[{"main_page": "https://arxiv.org/abs/2508.04224", "pdf": "https://arxiv.org/pdf/2508.04224.pdf", "title": "SplitGaussian: Reconstructing Dynamic Scenes via Visual Geometry Decomposition", "authors": "Jiahui Li , Shengeng Tang , Jingxuan He , Gang Huang , Zhangye Wang , Yantao Pan , Lechao Cheng", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "abstract": "Reconstructing dynamic 3D scenes from monocular video remains fundamentally challenging due to the need to jointly infer motion, structure, and appearance from limited observations. Existing dynamic scene reconstruction methods based on Gaussian Splatting often entangle static and dynamic elements in a shared representation, leading to motion leakage, geometric distortions, and temporal flickering. We identify that the root cause lies in the coupled modeling of geometry and appearance across time, which hampers both stability and interpretability. To address this, we propose \\textbf{SplitGaussian}, a novel framework that explicitly decomposes scene representations into static and dynamic components. By decoupling motion modeling from background geometry and allowing only the dynamic branch to deform over time, our method prevents motion artifacts in static regions while supporting view- and time-dependent appearance refinement. This disentangled design not only enhances temporal consistency and reconstruction fidelity but also accelerates convergence. Extensive experiments demonstrate that SplitGaussian outperforms prior state-of-the-art methods in rendering quality, geometric stability, and motion separation.", "comments": "", "project_url": "", "Relevancy score": 10, "Novelty score": 9, "Priority": "Must-read", "Reasons for match": "Gaussian Splatting \uae30\ubc18\uc758 dynamic scene \uc7ac\uad6c\uc131\uc5d0\uc11c static/dynamic \ubd84\ud574\ub294 NeRF\u00b7Gaussian Splatting\u00b7novel view synthesis \ubc0f temporal consistency \uc5f0\uad6c\uc5d0 \ub9e4\uc6b0 \uc9c1\uc811\uc801\uc774\uace0 \ub192\uc740 \uc601\ud5a5\ub825\uc744 \uac00\uc9d1\ub2c8\ub2e4.", "Venue": "", "Project page": "", "summarized_text": "Title: SplitGaussian: Reconstructing Dynamic Scenes via Visual Geometry Decomposition\nAuthors: Jiahui Li , Shengeng Tang , Jingxuan He , Gang Huang , Zhangye Wang , Yantao Pan , Lechao Cheng\nLink: https://arxiv.org/abs/2508.04224\nRelevancy score: 10\nNovelty score: 9\nPriority: Must-read\nReasons for match: Gaussian Splatting \uae30\ubc18\uc758 dynamic scene \uc7ac\uad6c\uc131\uc5d0\uc11c static/dynamic \ubd84\ud574\ub294 NeRF\u00b7Gaussian Splatting\u00b7novel view synthesis \ubc0f temporal consistency \uc5f0\uad6c\uc5d0 \ub9e4\uc6b0 \uc9c1\uc811\uc801\uc774\uace0 \ub192\uc740 \uc601\ud5a5\ub825\uc744 \uac00\uc9d1\ub2c8\ub2e4.\nVenue: \nProject page: \n"}, {"main_page": "https://arxiv.org/abs/2508.04147", "pdf": "https://arxiv.org/pdf/2508.04147.pdf", "title": "IDCNet: Guided Video Diffusion for Metric-Consistent RGBD Scene Generation with Precise Camera Control", "authors": "Lijuan Liu , Wenfa Li , Dongbo Zhang , Shuo Wang , Shaohui Jiao", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "abstract": "We present IDC-Net (Image-Depth Consistency Network), a novel framework designed to generate RGB-D video sequences under explicit camera trajectory control. Unlike approaches that treat RGB and depth generation separately, IDC-Net jointly synthesizes both RGB images and corresponding depth maps within a unified geometry-aware diffusion model. The joint learning framework strengthens spatial and geometric alignment across frames, enabling more precise camera control in the generated sequences. To support the training of this camera-conditioned model and ensure high geometric fidelity, we construct a camera-image-depth consistent dataset with metric-aligned RGB videos, depth maps, and accurate camera poses, which provides precise geometric supervision with notably improved inter-frame geometric consistency. Moreover, we introduce a geometry-aware transformer block that enables fine-grained camera control, enhancing control over the generated sequences. Extensive experiments show that IDC-Net achieves improvements over state-of-the-art approaches in both visual quality and geometric consistency of generated scene sequences. Notably, the generated RGB-D sequences can be directly feed for downstream 3D Scene reconstruction tasks without extra post-processing steps, showcasing the practical benefits of our joint learning framework. See more at this https URL .", "comments": "10 pages, 7 figures", "project_url": "", "Relevancy score": 9, "Novelty score": 9, "Priority": "Must-read", "Reasons for match": "RGB-D \ube44\ub514\uc624 \ub3d9\uc2dc \uc0dd\uc131 \ubc0f \uc815\ud655\ud55c \uce74\uba54\ub77c \uc81c\uc5b4\ub97c \uc704\ud55c geometry-aware diffusion/transformer \uc124\uacc4\ub85c, novel view synthesis\u00b7NeRF/3D reconstruction\uc5d0 \uc9c1\uc811 \uc751\uc6a9 \uac00\ub2a5\ud55c \uace0\uad00\uc2ec \uc5f0\uad6c\uc785\ub2c8\ub2e4.", "Venue": "", "Project page": "https://this_url.com", "summarized_text": "Title: IDCNet: Guided Video Diffusion for Metric-Consistent RGBD Scene Generation with Precise Camera Control\nAuthors: Lijuan Liu , Wenfa Li , Dongbo Zhang , Shuo Wang , Shaohui Jiao\nLink: https://arxiv.org/abs/2508.04147\nRelevancy score: 9\nNovelty score: 9\nPriority: Must-read\nReasons for match: RGB-D \ube44\ub514\uc624 \ub3d9\uc2dc \uc0dd\uc131 \ubc0f \uc815\ud655\ud55c \uce74\uba54\ub77c \uc81c\uc5b4\ub97c \uc704\ud55c geometry-aware diffusion/transformer \uc124\uacc4\ub85c, novel view synthesis\u00b7NeRF/3D reconstruction\uc5d0 \uc9c1\uc811 \uc751\uc6a9 \uac00\ub2a5\ud55c \uace0\uad00\uc2ec \uc5f0\uad6c\uc785\ub2c8\ub2e4.\nVenue: \nProject page: https://this_url.com\n"}, {"main_page": "https://arxiv.org/abs/2508.04211", "pdf": "https://arxiv.org/pdf/2508.04211.pdf", "title": "What Holds Back Open-Vocabulary Segmentation?", "authors": "Josip \u0160ari\u0107 , Ivan Martinovi\u0107 , Matej Kristan , Sini\u0161a \u0160egvi\u0107", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "abstract": "Standard segmentation setups are unable to deliver models that can recognize concepts outside the training taxonomy. Open-vocabulary approaches promise to close this gap through language-image pretraining on billions of image-caption pairs. Unfortunately, we observe that the promise is not delivered due to several bottlenecks that have caused the performance to plateau for almost two years. This paper proposes novel oracle components that identify and decouple these bottlenecks by taking advantage of the groundtruth information. The presented validation experiments deliver important empirical findings that provide a deeper insight into the failures of open-vocabulary models and suggest prominent approaches to unlock the future research.", "comments": "Accepted for publication at ICCV 25 Workshop: What is Next in Multimodal Foundation Models?", "project_url": "", "Relevancy score": 9, "Novelty score": 8, "Priority": "Must-read", "Reasons for match": "open-vocabulary segmentation\uc758 \ubcd1\ubaa9\uc744 \uc2e4\ud5d8\uc801\uc73c\ub85c \ud574\uc11d\u00b7\ubd84\ud574\ud558\uc5ec \ud5a5\ud6c4 VLM \uae30\ubc18 \uc138\ubd84\ud654\uc640 \ub300\uaddc\ubaa8 \uc5b8\uc5b4-\ube44\uc804 \uc0ac\uc804\ud559\uc2b5 \uc5f0\uad6c\uc5d0 \ub300\ud55c \uae4a\uc740 \ud1b5\ucc30\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4.", "Venue": "ICCV Workshop", "Project page": "", "summarized_text": "Title: What Holds Back Open-Vocabulary Segmentation?\nAuthors: Josip \u0160ari\u0107 , Ivan Martinovi\u0107 , Matej Kristan , Sini\u0161a \u0160egvi\u0107\nLink: https://arxiv.org/abs/2508.04211\nRelevancy score: 9\nNovelty score: 8\nPriority: Must-read\nReasons for match: open-vocabulary segmentation\uc758 \ubcd1\ubaa9\uc744 \uc2e4\ud5d8\uc801\uc73c\ub85c \ud574\uc11d\u00b7\ubd84\ud574\ud558\uc5ec \ud5a5\ud6c4 VLM \uae30\ubc18 \uc138\ubd84\ud654\uc640 \ub300\uaddc\ubaa8 \uc5b8\uc5b4-\ube44\uc804 \uc0ac\uc804\ud559\uc2b5 \uc5f0\uad6c\uc5d0 \ub300\ud55c \uae4a\uc740 \ud1b5\ucc30\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4.\nVenue: ICCV Workshop\nProject page: \n"}, {"main_page": "https://arxiv.org/abs/2508.04297", "pdf": "https://arxiv.org/pdf/2508.04297.pdf", "title": "MuGS: Multi-Baseline Generalizable Gaussian Splatting Reconstruction", "authors": "Yaopeng Lou , Liao Shen , Tianqi Liu , Jiaqi Li , Zihao Huang , Huiqiang Sun , Zhiguo Cao", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "abstract": "We present Multi-Baseline Gaussian Splatting (MuRF), a generalized feed-forward approach for novel view synthesis that effectively handles diverse baseline settings, including sparse input views with both small and large baselines. Specifically, we integrate features from Multi-View Stereo (MVS) and Monocular Depth Estimation (MDE) to enhance feature representations for generalizable reconstruction. Next, We propose a projection-and-sampling mechanism for deep depth fusion, which constructs a fine probability volume to guide the regression of the feature map. Furthermore, We introduce a reference-view loss to improve geometry and optimization efficiency. We leverage 3D Gaussian representations to accelerate training and inference time while enhancing rendering quality. MuRF achieves state-of-the-art performance across multiple baseline settings and diverse scenarios ranging from simple objects (DTU) to complex indoor and outdoor scenes (RealEstate10K). We also demonstrate promising zero-shot performance on the LLFF and Mip-NeRF 360 datasets.", "comments": "This work is accepted by ICCV 2025", "project_url": "", "Relevancy score": 9, "Novelty score": 8, "Priority": "Must-read", "Reasons for match": "Gaussian Splatting \uae30\ubc18\uc758 multi-baseline novel view synthesis\ub294 NeRF/\ub80c\ub354\ub9c1\u00b73D \uc7ac\uad6c\uc131\uc5d0 \uc9c1\uc811 \uc5f0\uad00\ub418\uba70 ICCV \ucc44\ud0dd\uc73c\ub85c SOTA \uac00\ub2a5\uc131\uc774 \ub192\uc2b5\ub2c8\ub2e4.", "Venue": "ICCV", "Project page": "", "summarized_text": "Title: MuGS: Multi-Baseline Generalizable Gaussian Splatting Reconstruction\nAuthors: Yaopeng Lou , Liao Shen , Tianqi Liu , Jiaqi Li , Zihao Huang , Huiqiang Sun , Zhiguo Cao\nLink: https://arxiv.org/abs/2508.04297\nRelevancy score: 9\nNovelty score: 8\nPriority: Must-read\nReasons for match: Gaussian Splatting \uae30\ubc18\uc758 multi-baseline novel view synthesis\ub294 NeRF/\ub80c\ub354\ub9c1\u00b73D \uc7ac\uad6c\uc131\uc5d0 \uc9c1\uc811 \uc5f0\uad00\ub418\uba70 ICCV \ucc44\ud0dd\uc73c\ub85c SOTA \uac00\ub2a5\uc131\uc774 \ub192\uc2b5\ub2c8\ub2e4.\nVenue: ICCV\nProject page: \n"}, {"main_page": "https://arxiv.org/abs/2508.04508", "pdf": "https://arxiv.org/pdf/2508.04508.pdf", "title": "Surf3R: Rapid Surface Reconstruction from Sparse RGB Views in Seconds", "authors": "Haodong Zhu , Changbai Li , Yangyang Ren , Zichao Feng , Xuhui Liu , Hanlin Chen , Xiantong Zhen , Baochang Zhang", "subjects": "Graphics (cs.GR) ; Computer Vision and Pattern Recognition (cs.CV)", "abstract": "Current multi-view 3D reconstruction methods rely on accurate camera calibration and pose estimation, requiring complex and time-intensive pre-processing that hinders their practical deployment. To address this challenge, we introduce Surf3R, an end-to-end feedforward approach that reconstructs 3D surfaces from sparse views without estimating camera poses and completes an entire scene in under 10 seconds. Our method employs a multi-branch and multi-view decoding architecture in which multiple reference views jointly guide the reconstruction process. Through the proposed branch-wise processing, cross-view attention, and inter-branch fusion, the model effectively captures complementary geometric cues without requiring camera calibration. Moreover, we introduce a D-Normal regularizer based on an explicit 3D Gaussian representation for surface reconstruction. It couples surface normals with other geometric parameters to jointly optimize the 3D geometry, significantly improving 3D consistency and surface detail accuracy. Experimental results demonstrate that Surf3R achieves state-of-the-art performance on multiple surface reconstruction metrics on ScanNet++ and Replica datasets, exhibiting excellent generalization and efficiency.", "comments": "", "project_url": "", "Relevancy score": 9, "Novelty score": 8, "Priority": "Must-read", "Reasons for match": "sparse RGB views\uc5d0\uc11c \uce74\uba54\ub77c \ud3ec\uc988 \ucd94\uc815 \uc5c6\uc774 \uc218\ucd08 \ub0b4\uc5d0 surface reconstruction\uc744 \uc218\ud589\ud558\ub294 end-to-end feedforward \uae30\ubc95\uc73c\ub85c, multi-view 3D reconstruction, efficiency, novel architecture \uce21\uba74\uc5d0\uc11c \ub9e4\uc6b0 \uad00\ub828\uc131\uc774 \ub192\ub2e4.", "Venue": "", "Project page": "", "summarized_text": "Title: Surf3R: Rapid Surface Reconstruction from Sparse RGB Views in Seconds\nAuthors: Haodong Zhu , Changbai Li , Yangyang Ren , Zichao Feng , Xuhui Liu , Hanlin Chen , Xiantong Zhen , Baochang Zhang\nLink: https://arxiv.org/abs/2508.04508\nRelevancy score: 9\nNovelty score: 8\nPriority: Must-read\nReasons for match: sparse RGB views\uc5d0\uc11c \uce74\uba54\ub77c \ud3ec\uc988 \ucd94\uc815 \uc5c6\uc774 \uc218\ucd08 \ub0b4\uc5d0 surface reconstruction\uc744 \uc218\ud589\ud558\ub294 end-to-end feedforward \uae30\ubc95\uc73c\ub85c, multi-view 3D reconstruction, efficiency, novel architecture \uce21\uba74\uc5d0\uc11c \ub9e4\uc6b0 \uad00\ub828\uc131\uc774 \ub192\ub2e4.\nVenue: \nProject page: \n"}, {"main_page": "https://arxiv.org/abs/2508.04581", "pdf": "https://arxiv.org/pdf/2508.04581.pdf", "title": "Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning", "authors": "Magauiya Zhussip , Dmitriy Shopkhoev , Ammar Ali , Stamatios Lefkimmiatis", "subjects": "Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI)", "abstract": "Large language models (LLMs) have revolutionized AI applications, yet their high computational and memory demands hinder their widespread deployment. Existing compression techniques focus on intra-block optimizations (e.g. low-rank approximation, attention head pruning), while the repetitive layered structure of transformers implies significant inter-block redundancy - a dimension largely unexplored beyond key-value (KV) caching. Inspired by dictionary learning in CNNs, we propose a framework for structured weight sharing across transformer layers. Our approach decomposes attention projection matrices into shared dictionary atoms, reducing the attention module's parameters by 66.7% while achieving on-par performance. Unlike complex methods requiring distillation or architectural changes, MASA (Matrix Atom Sharing in Attention) operates as a drop-in replacement - trained with standard optimizers - and represents each layer's weights as linear combinations of shared matrix atoms. Experiments across scales (100M-700M parameters) show that MASA achieves better benchmark accuracy and perplexity than grouped-query attention (GQA), low-rank baselines and recently proposed Repeat-all-over/Sequential sharing at comparable parameter budgets. Ablation studies confirm robustness to the dictionary size and the efficacy of shared representations in capturing cross-layer statistical regularities. Extending to Vision Transformers (ViT), MASA matches performance metrics on image classification and detection tasks with 66.7% fewer attention parameters. By combining dictionary learning strategies with transformer efficiency, MASA offers a scalable blueprint for parameter-efficient models without sacrificing performance. Finally, we investigate the possibility of employing MASA on pretrained LLMs to reduce their number of parameters without experiencing any significant drop in their performance.", "comments": "", "project_url": "", "Relevancy score": 9, "Novelty score": 8, "Priority": "Must-read", "Reasons for match": "Transformer \uce35 \uac04 weight sharing\uc744 \ud1b5\ud55c \ud30c\ub77c\ubbf8\ud130 \ud6a8\uc728\ud654(MASA)\ub294 novel architectures \ubc0f \ubaa8\ub378 \uc555\ucd95(vision transformer \ud3ec\ud568)\uc5d0 \uc9c1\uc811\uc801\uc73c\ub85c \uc5f0\uad00\ub418\uc5b4 \uc911\uc694\ud569\ub2c8\ub2e4.", "Venue": "", "Project page": "", "summarized_text": "Title: Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning\nAuthors: Magauiya Zhussip , Dmitriy Shopkhoev , Ammar Ali , Stamatios Lefkimmiatis\nLink: https://arxiv.org/abs/2508.04581\nRelevancy score: 9\nNovelty score: 8\nPriority: Must-read\nReasons for match: Transformer \uce35 \uac04 weight sharing\uc744 \ud1b5\ud55c \ud30c\ub77c\ubbf8\ud130 \ud6a8\uc728\ud654(MASA)\ub294 novel architectures \ubc0f \ubaa8\ub378 \uc555\ucd95(vision transformer \ud3ec\ud568)\uc5d0 \uc9c1\uc811\uc801\uc73c\ub85c \uc5f0\uad00\ub418\uc5b4 \uc911\uc694\ud569\ub2c8\ub2e4.\nVenue: \nProject page: \n"}, {"main_page": "https://arxiv.org/abs/2508.04659", "pdf": "https://arxiv.org/pdf/2508.04659.pdf", "title": "PixCuboid: Room Layout Estimation from Multi-view Featuremetric Alignment", "authors": "Gustav Hanning , Kalle \u00c5str\u00f6m , Viktor Larsson", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "abstract": "Coarse room layout estimation provides important geometric cues for many downstream tasks. Current state-of-the-art methods are predominantly based on single views and often assume panoramic images. We introduce PixCuboid, an optimization-based approach for cuboid-shaped room layout estimation, which is based on multi-view alignment of dense deep features. By training with the optimization end-to-end, we learn feature maps that yield large convergence basins and smooth loss landscapes in the alignment. This allows us to initialize the room layout using simple heuristics. For the evaluation we propose two new benchmarks based on ScanNet++ and 2D-3D-Semantics, with manually verified ground truth 3D cuboids. In thorough experiments we validate our approach and significantly outperform the competition. Finally, while our network is trained with single cuboids, the flexibility of the optimization-based approach allow us to easily extend to multi-room estimation, e.g. larger apartments or offices. Code and model weights are available at this https URL .", "comments": "Accepted at the ICCV 2025 Workshop on Large Scale Cross Device Localization", "project_url": "", "Relevancy score": 9, "Novelty score": 8, "Priority": "Must-read", "Reasons for match": "multi-view feature-metric alignment \uae30\ubc18\uc758 3D \uc2e4\ub0b4 \ub808\uc774\uc544\uc6c3 \ucd94\uc815\uc73c\ub85c multi-view/3D \uc7ac\uad6c\uc131\u00b7\ud45c\ud604\ud559\uc2b5 \ubc0f \ucd5c\uc801\ud654 \uae30\ubc18 \ubc29\ubc95\ub860\uc5d0 \uc9c1\uc811 \uc5f0\uad00\ub429\ub2c8\ub2e4.", "Venue": "ICCV Workshop", "Project page": "https://github.com/", "summarized_text": "Title: PixCuboid: Room Layout Estimation from Multi-view Featuremetric Alignment\nAuthors: Gustav Hanning , Kalle \u00c5str\u00f6m , Viktor Larsson\nLink: https://arxiv.org/abs/2508.04659\nRelevancy score: 9\nNovelty score: 8\nPriority: Must-read\nReasons for match: multi-view feature-metric alignment \uae30\ubc18\uc758 3D \uc2e4\ub0b4 \ub808\uc774\uc544\uc6c3 \ucd94\uc815\uc73c\ub85c multi-view/3D \uc7ac\uad6c\uc131\u00b7\ud45c\ud604\ud559\uc2b5 \ubc0f \ucd5c\uc801\ud654 \uae30\ubc18 \ubc29\ubc95\ub860\uc5d0 \uc9c1\uc811 \uc5f0\uad00\ub429\ub2c8\ub2e4.\nVenue: ICCV Workshop\nProject page: https://github.com/\n"}, {"main_page": "https://arxiv.org/abs/2508.04705", "pdf": "https://arxiv.org/pdf/2508.04705.pdf", "title": "Occupancy Learning with Spatiotemporal Memory", "authors": "Ziyang Leng , Jiawei Yang , Wenlong Yi , Bolei Zhou", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "abstract": "3D occupancy becomes a promising perception representation for autonomous driving to model the surrounding environment at a fine-grained scale. However, it remains challenging to efficiently aggregate 3D occupancy over time across multiple input frames due to the high processing cost and the uncertainty and dynamics of voxels. To address this issue, we propose ST-Occ, a scene-level occupancy representation learning framework that effectively learns the spatiotemporal feature with temporal consistency. ST-Occ consists of two core designs: a spatiotemporal memory that captures comprehensive historical information and stores it efficiently through a scene-level representation and a memory attention that conditions the current occupancy representation on the spatiotemporal memory with a model of uncertainty and dynamic awareness. Our method significantly enhances the spatiotemporal representation learned for 3D occupancy prediction tasks by exploiting the temporal dependency between multi-frame inputs. Experiments show that our approach outperforms the state-of-the-art methods by a margin of 3 mIoU and reduces the temporal inconsistency by 29%.", "comments": "Accepted to ICCV2025. Project website: this https URL", "project_url": "", "Relevancy score": 9, "Novelty score": 8, "Priority": "Must-read", "Reasons for match": "ST-Occ\ub294 3D occupancy\uc640 spatiotemporal memory\ub97c \ub2e4\ub8e8\uba70, 3D perception, multi-frame aggregation \ubc0f ICCV \ucc44\ud0dd\uc73c\ub85c \ubcf8\uc778\uc758 3D/representation/architectures \uc5f0\uad6c\uc5d0 \ub192\uc740 \uad00\ub828\uc131\uc774 \uc788\uc2b5\ub2c8\ub2e4.", "Venue": "ICCV2025", "Project page": "https://example.com", "summarized_text": "Title: Occupancy Learning with Spatiotemporal Memory\nAuthors: Ziyang Leng , Jiawei Yang , Wenlong Yi , Bolei Zhou\nLink: https://arxiv.org/abs/2508.04705\nRelevancy score: 9\nNovelty score: 8\nPriority: Must-read\nReasons for match: ST-Occ\ub294 3D occupancy\uc640 spatiotemporal memory\ub97c \ub2e4\ub8e8\uba70, 3D perception, multi-frame aggregation \ubc0f ICCV \ucc44\ud0dd\uc73c\ub85c \ubcf8\uc778\uc758 3D/representation/architectures \uc5f0\uad6c\uc5d0 \ub192\uc740 \uad00\ub828\uc131\uc774 \uc788\uc2b5\ub2c8\ub2e4.\nVenue: ICCV2025\nProject page: https://example.com\n"}, {"main_page": "https://arxiv.org/abs/2508.04078", "pdf": "https://arxiv.org/pdf/2508.04078.pdf", "title": "RLGS: Reinforcement Learning-Based Adaptive Hyperparameter Tuning for Gaussian Splatting", "authors": "Zhan Li , Huangying Zhan , Changyang Li , Qingan Yan , Yi Xu", "subjects": "Graphics (cs.GR) ; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)", "abstract": "Hyperparameter tuning in 3D Gaussian Splatting (3DGS) is a labor-intensive and expert-driven process, often resulting in inconsistent reconstructions and suboptimal results. We propose RLGS, a plug-and-play reinforcement learning framework for adaptive hyperparameter tuning in 3DGS through lightweight policy modules, dynamically adjusting critical hyperparameters such as learning rates and densification thresholds. The framework is model-agnostic and seamlessly integrates into existing 3DGS pipelines without architectural modifications. We demonstrate its generalization ability across multiple state-of-the-art 3DGS variants, including Taming-3DGS and 3DGS-MCMC, and validate its robustness across diverse datasets. RLGS consistently enhances rendering quality. For example, it improves Taming-3DGS by 0.7dB PSNR on the Tanks and Temple (TNT) dataset, under a fixed Gaussian budget, and continues to yield gains even when baseline performance saturates. Our results suggest that RLGS provides an effective and general solution for automating hyperparameter tuning in 3DGS training, bridging a gap in applying reinforcement learning to 3DGS.", "comments": "14 pages, 9 figures", "project_url": "", "Relevancy score": 8, "Novelty score": 6, "Priority": "Must-read", "Reasons for match": "\ubcf8 \ub17c\ubb38\uc740 3D Gaussian Splatting\uc758 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \uc790\ub3d9\ud654\uc5d0 RL\uc744 \uc801\uc6a9\ud558\uc5ec 3D \uc7ac\uad6c\uc131 \ubc0f \ub80c\ub354\ub9c1 \ud488\uc9c8\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ubbc0\ub85c 3D Vision \ubc0f training efficiency\uc640 \uc9c1\uc811\uc801\uc73c\ub85c \uc5f0\uad00\ub41c\ub2e4.", "Venue": "", "Project page": "", "summarized_text": "Title: RLGS: Reinforcement Learning-Based Adaptive Hyperparameter Tuning for Gaussian Splatting\nAuthors: Zhan Li , Huangying Zhan , Changyang Li , Qingan Yan , Yi Xu\nLink: https://arxiv.org/abs/2508.04078\nRelevancy score: 8\nNovelty score: 6\nPriority: Must-read\nReasons for match: \ubcf8 \ub17c\ubb38\uc740 3D Gaussian Splatting\uc758 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \uc790\ub3d9\ud654\uc5d0 RL\uc744 \uc801\uc6a9\ud558\uc5ec 3D \uc7ac\uad6c\uc131 \ubc0f \ub80c\ub354\ub9c1 \ud488\uc9c8\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ubbc0\ub85c 3D Vision \ubc0f training efficiency\uc640 \uc9c1\uc811\uc801\uc73c\ub85c \uc5f0\uad00\ub41c\ub2e4.\nVenue: \nProject page: \n"}, {"main_page": "https://arxiv.org/abs/2508.04122", "pdf": "https://arxiv.org/pdf/2508.04122.pdf", "title": "Conditional Latent Diffusion Models for Zero-Shot Instance Segmentation", "authors": "Maximilian Ulmer , Wout Boerdijk , Rudolph Triebel , Maximilian Durner", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "abstract": "This paper presents OC-DiT, a novel class of diffusion models designed for object-centric prediction, and applies it to zero-shot instance segmentation. We propose a conditional latent diffusion framework that generates instance masks by conditioning the generative process on object templates and image features within the diffusion model's latent space. This allows our model to effectively disentangle object instances through the diffusion process, which is guided by visual object descriptors and localized image cues. Specifically, we introduce two model variants: a coarse model for generating initial object instance proposals, and a refinement model that refines all proposals in parallel. We train these models on a newly created, large-scale synthetic dataset comprising thousands of high-quality object meshes. Remarkably, our model achieves state-of-the-art performance on multiple challenging real-world benchmarks, without requiring any retraining on target data. Through comprehensive ablation studies, we demonstrate the potential of diffusion models for instance segmentation tasks.", "comments": "ICCV 2025", "project_url": "", "Relevancy score": 8, "Novelty score": 8, "Priority": "Must-read", "Reasons for match": "latent diffusion\uc744 \uc778\uc2a4\ud134\uc2a4 \ubd84\ud560\uc5d0 \uc801\uc6a9\ud558\uace0 \uc81c\ub85c\uc0f7 \uc131\ub2a5 \ubc0f \ub300\uaddc\ubaa8 \ud569\uc131 \uba54\uc26c \ub370\uc774\ud130\uc14b\uc73c\ub85c 3D/\uc774\ubbf8\uc9c0-\uae30\ubc18 \uc778\uc2a4\ud134\uc2a4 \ubd84\ub9ac \uc5f0\uad6c(\ud2b9\ud788 diffusion \uae30\ubc18 segmentation \ubc0f 3D \uad00\ub828 generative \ubaa8\ub378)\uc5d0 \uc9c1\uc811 \uc5f0\uad00\ub429\ub2c8\ub2e4.", "Venue": "ICCV 2025", "Project page": "", "summarized_text": "Title: Conditional Latent Diffusion Models for Zero-Shot Instance Segmentation\nAuthors: Maximilian Ulmer , Wout Boerdijk , Rudolph Triebel , Maximilian Durner\nLink: https://arxiv.org/abs/2508.04122\nRelevancy score: 8\nNovelty score: 8\nPriority: Must-read\nReasons for match: latent diffusion\uc744 \uc778\uc2a4\ud134\uc2a4 \ubd84\ud560\uc5d0 \uc801\uc6a9\ud558\uace0 \uc81c\ub85c\uc0f7 \uc131\ub2a5 \ubc0f \ub300\uaddc\ubaa8 \ud569\uc131 \uba54\uc26c \ub370\uc774\ud130\uc14b\uc73c\ub85c 3D/\uc774\ubbf8\uc9c0-\uae30\ubc18 \uc778\uc2a4\ud134\uc2a4 \ubd84\ub9ac \uc5f0\uad6c(\ud2b9\ud788 diffusion \uae30\ubc18 segmentation \ubc0f 3D \uad00\ub828 generative \ubaa8\ub378)\uc5d0 \uc9c1\uc811 \uc5f0\uad00\ub429\ub2c8\ub2e4.\nVenue: ICCV 2025\nProject page: \n"}, {"main_page": "https://arxiv.org/abs/2508.04201", "pdf": "https://arxiv.org/pdf/2508.04201.pdf", "title": "ViFP: A Framework for Visual False Positive Detection to Enhance Reasoning Reliability in VLMs", "authors": "Ben Zhang , LuLu Yu , Lei Gao , Jing Liu , QuanJiang Guo , Hui Gao", "subjects": "Computer Vision and Pattern Recognition (cs.CV) ; Artificial Intelligence (cs.AI)", "abstract": "In visual-language model (VLM) reasoning, false positive(FP) reasoning occurs when a model generates a correct answer but follows an incorrect reasoning path. Existing methods based on specific multi-step reasoning datasets and reinforcement learning strategies, leading to high training costs and limited generalization. In this work, we propose ViFP, a general framework for enhancing visual reasoning reliability. It improves both answer accuracy and reasoning soundness by detecting FPs. ViFP tackles the limitations of dataset dependency and poor generalization by constructing sub-question templates grounded in the core dimensions of visual reasoning, such as object localization, characteristic description, and object discovery. ViFP then builds effective reasoning paths via multi-turn QA to improve reasoning accuracy. Meanwhile, ViFP dynamically analyzes the consistency of reasoning path to identify potential FPs, and introduces a targeted chain-of-thought (CoT) mechanism that adaptively guides both FP and non-FP samples. Thereby reducing logical errors in the reasoning path while preserving accuracy. Finally, we introduce a reliability evaluation metric-VoC, which integrates answer accuracy and the FP rate, providing a quantitative tool to assess whether a VLM not only answers correctly, but also reasons reliably. Our experiments on closed-source VLMs show that ViFP consistently improves performance across three datasets: A-OKVQA, OKVQA, and FVQA. On A-OKVQA, ViFP improves accuracy by up to 5.4%, surpassing the previous state-of-the-art by 4.3%, and significantly reduces the number of FPs, validating its benefits in enhancing reasoning reliability.", "comments": "", "project_url": "", "Relevancy score": 8, "Novelty score": 7, "Priority": "Must-read", "Reasons for match": "VLM\uc758 reasoning \uc2e0\ub8b0\uc131 \ud5a5\uc0c1\uc744 \uc704\ud55c FP \ud0d0\uc9c0\uc640 adaptive chain-of-thought \uae30\ubc95\uc740 vision-language reasoning \ubc0f VLM \uc548\uc804\uc131\u00b7\ud574\uc11d\uc131 \uc5f0\uad6c\uc5d0 \uc9c1\uc811\uc801\uc73c\ub85c \uae30\uc5ec\ud569\ub2c8\ub2e4.", "Venue": "", "Project page": "", "summarized_text": "Title: ViFP: A Framework for Visual False Positive Detection to Enhance Reasoning Reliability in VLMs\nAuthors: Ben Zhang , LuLu Yu , Lei Gao , Jing Liu , QuanJiang Guo , Hui Gao\nLink: https://arxiv.org/abs/2508.04201\nRelevancy score: 8\nNovelty score: 7\nPriority: Must-read\nReasons for match: VLM\uc758 reasoning \uc2e0\ub8b0\uc131 \ud5a5\uc0c1\uc744 \uc704\ud55c FP \ud0d0\uc9c0\uc640 adaptive chain-of-thought \uae30\ubc95\uc740 vision-language reasoning \ubc0f VLM \uc548\uc804\uc131\u00b7\ud574\uc11d\uc131 \uc5f0\uad6c\uc5d0 \uc9c1\uc811\uc801\uc73c\ub85c \uae30\uc5ec\ud569\ub2c8\ub2e4.\nVenue: \nProject page: \n"}, {"main_page": "https://arxiv.org/abs/2508.04227", "pdf": "https://arxiv.org/pdf/2508.04227.pdf", "title": "Continual Learning for VLMs: A Survey and Taxonomy Beyond Forgetting", "authors": "Yuyang Liu , Qiuhe Hong , Linlan Huang , Alexandra Gomez-Villa , Dipam Goswami , Xialei Liu , Joost van de Weijer , Yonghong Tian", "subjects": "Computer Vision and Pattern Recognition (cs.CV) ; Machine Learning (cs.LG)", "abstract": "Vision-language models (VLMs) have achieved impressive performance across diverse multimodal tasks by leveraging large-scale pre-training. However, enabling them to learn continually from non-stationary data remains a major challenge, as their cross-modal alignment and generalization capabilities are particularly vulnerable to catastrophic forgetting. Unlike traditional unimodal continual learning (CL), VLMs face unique challenges such as cross-modal feature drift, parameter interference due to shared architectures, and zero-shot capability erosion. This survey offers the first focused and systematic review of continual learning for VLMs (VLM-CL). We begin by identifying the three core failure modes that degrade performance in VLM-CL. Based on these, we propose a challenge-driven taxonomy that maps solutions to their target problems: (1) \\textit{Multi-Modal Replay Strategies} address cross-modal drift through explicit or implicit memory mechanisms; (2) \\textit{Cross-Modal Regularization} preserves modality alignment during updates; and (3) \\textit{Parameter-Efficient Adaptation} mitigates parameter interference with modular or low-rank updates. We further analyze current evaluation protocols, datasets, and metrics, highlighting the need for better benchmarks that capture VLM-specific forgetting and compositional generalization. Finally, we outline open problems and future directions, including continual pre-training and compositional zero-shot learning. This survey aims to serve as a comprehensive and diagnostic reference for researchers developing lifelong vision-language systems. All resources are available at: this https URL .", "comments": "", "project_url": "", "Relevancy score": 8, "Novelty score": 6, "Priority": "Must-read", "Reasons for match": "VLM\uc758 continual learning\uc744 \uccb4\uacc4\uc801\uc73c\ub85c \uc815\ub9ac\ud558\uc5ec cross-modal replay\u00b7regularization\u00b7parameter-efficient adaptation \ub4f1 VLM \ud2b9\ud654 \uc9c0\uc18d \ud559\uc2b5 \ubb38\uc81c\uc640 \ud574\uacb0\ucc45\uc744 \ud3ec\uad04\uc801\uc73c\ub85c \ub2e4\ub8f9\ub2c8\ub2e4.", "Venue": "", "Project page": "https://github.com/", "summarized_text": "Title: Continual Learning for VLMs: A Survey and Taxonomy Beyond Forgetting\nAuthors: Yuyang Liu , Qiuhe Hong , Linlan Huang , Alexandra Gomez-Villa , Dipam Goswami , Xialei Liu , Joost van de Weijer , Yonghong Tian\nLink: https://arxiv.org/abs/2508.04227\nRelevancy score: 8\nNovelty score: 6\nPriority: Must-read\nReasons for match: VLM\uc758 continual learning\uc744 \uccb4\uacc4\uc801\uc73c\ub85c \uc815\ub9ac\ud558\uc5ec cross-modal replay\u00b7regularization\u00b7parameter-efficient adaptation \ub4f1 VLM \ud2b9\ud654 \uc9c0\uc18d \ud559\uc2b5 \ubb38\uc81c\uc640 \ud574\uacb0\ucc45\uc744 \ud3ec\uad04\uc801\uc73c\ub85c \ub2e4\ub8f9\ub2c8\ub2e4.\nVenue: \nProject page: https://github.com/\n"}, {"main_page": "https://arxiv.org/abs/2508.04236", "pdf": "https://arxiv.org/pdf/2508.04236.pdf", "title": "PIS3R: Very Large Parallax Image Stitching via Deep 3D Reconstruction", "authors": "Muhua Zhu , Xinhao Jin , Chengbo Wang , Yongcong Zhang , Yifei Xue , Tie Ji , Yizhen Lao", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "abstract": "Image stitching aim to align two images taken from different viewpoints into one seamless, wider image. However, when the 3D scene contains depth variations and the camera baseline is significant, noticeable parallax occurs-meaning the relative positions of scene elements differ substantially between views. Most existing stitching methods struggle to handle such images with large parallax effectively. To address this challenge, in this paper, we propose an image stitching solution called PIS3R that is robust to very large parallax based on the novel concept of deep 3D reconstruction. First, we apply visual geometry grounded transformer to two input images with very large parallax to obtain both intrinsic and extrinsic parameters, as well as the dense 3D scene reconstruction. Subsequently, we reproject reconstructed dense point cloud onto a designated reference view using the recovered camera parameters, achieving pixel-wise alignment and generating an initial stitched image. Finally, to further address potential artifacts such as holes or noise in the initial stitching, we propose a point-conditioned image diffusion module to obtain the refined this http URL with existing methods, our solution is very large parallax tolerant and also provides results that fully preserve the geometric integrity of all pixels in the 3D photogrammetric context, enabling direct applicability to downstream 3D vision tasks such as SfM. Experimental results demonstrate that the proposed algorithm provides accurate stitching results for images with very large parallax, and outperforms the existing methods qualitatively and quantitatively.", "comments": "", "project_url": "", "Relevancy score": 8, "Novelty score": 7, "Priority": "Must-read", "Reasons for match": "\ub450 \uc774\ubbf8\uc9c0\uc758 \ub300\ud3ed\ud55c \ud328\ub7f4\ub799\uc2a4 \ubb38\uc81c\uc5d0 \ub300\ud574 dense 3D reconstruction + transformer \uae30\ubc18 \uce74\uba54\ub77c \ubcf5\uc6d0 \ubc0f diffusion\uc73c\ub85c \uc815\uc81c\ud558\ub294 \ud30c\uc774\ud504\ub77c\uc778\uc744 \uc81c\uc548\ud558\uc5ec multi-view 3D \uc7ac\uad6c\uc131\uacfc novel view/stitching \uad00\ub828 \ud575\uc2ec \uc8fc\uc81c\uc640 \ubc00\uc811\ud568.", "Venue": "", "Project page": "", "summarized_text": "Title: PIS3R: Very Large Parallax Image Stitching via Deep 3D Reconstruction\nAuthors: Muhua Zhu , Xinhao Jin , Chengbo Wang , Yongcong Zhang , Yifei Xue , Tie Ji , Yizhen Lao\nLink: https://arxiv.org/abs/2508.04236\nRelevancy score: 8\nNovelty score: 7\nPriority: Must-read\nReasons for match: \ub450 \uc774\ubbf8\uc9c0\uc758 \ub300\ud3ed\ud55c \ud328\ub7f4\ub799\uc2a4 \ubb38\uc81c\uc5d0 \ub300\ud574 dense 3D reconstruction + transformer \uae30\ubc18 \uce74\uba54\ub77c \ubcf5\uc6d0 \ubc0f diffusion\uc73c\ub85c \uc815\uc81c\ud558\ub294 \ud30c\uc774\ud504\ub77c\uc778\uc744 \uc81c\uc548\ud558\uc5ec multi-view 3D \uc7ac\uad6c\uc131\uacfc novel view/stitching \uad00\ub828 \ud575\uc2ec \uc8fc\uc81c\uc640 \ubc00\uc811\ud568.\nVenue: \nProject page: \n"}, {"main_page": "https://arxiv.org/abs/2508.04286", "pdf": "https://arxiv.org/pdf/2508.04286.pdf", "title": "PKSS-Align: Robust Point Cloud Registration on Pre-Kendall Shape Space", "authors": "Chenlei Lv , Hui Huang", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "abstract": "Point cloud registration is a classical topic in the field of 3D Vision and Computer Graphics. Generally, the implementation of registration is typically sensitive to similarity transformations (translation, scaling, and rotation), noisy points, and incomplete geometric structures. Especially, the non-uniform scales and defective parts of point clouds increase probability of struck local optima in registration task. In this paper, we propose a robust point cloud registration PKSS-Align that can handle various influences, including similarity transformations, non-uniform densities, random noisy points, and defective parts. The proposed method measures shape feature-based similarity between point clouds on the Pre-Kendall shape space (PKSS), \\textcolor{black}{which is a shape measurement-based scheme and doesn't require point-to-point or point-to-plane metric.} The employed measurement can be regarded as the manifold metric that is robust to various representations in the Euclidean coordinate system. Benefited from the measurement, the transformation matrix can be directly generated for point clouds with mentioned influences at the same time. The proposed method does not require data training and complex feature encoding. Based on a simple parallel acceleration, it can achieve significant improvement for efficiency and feasibility in practice. Experiments demonstrate that our method outperforms the relevant state-of-the-art methods.", "comments": "15 pages, 15 figures, and will be published in IEEE TVCG", "project_url": "", "Relevancy score": 8, "Novelty score": 7, "Priority": "Must-read", "Reasons for match": "\ud3ec\uc778\ud2b8\ud074\ub77c\uc6b0\ub4dc \uc815\ud569(point cloud registration)\uc740 3D \ube44\uc804 \ud575\uc2ec \uc8fc\uc81c\uc774\uba70, \uacac\uace0\ud55c PKSS \uae30\ubc18 \uc811\uadfc\uacfc TVCG \ucd9c\ud310\uc740 \uc2e4\ubb34\uc801\u00b7\uc774\ub860\uc801 \uac00\uce58\uac00 \ud07d\ub2c8\ub2e4.", "Venue": "IEEE TVCG", "Project page": "", "summarized_text": "Title: PKSS-Align: Robust Point Cloud Registration on Pre-Kendall Shape Space\nAuthors: Chenlei Lv , Hui Huang\nLink: https://arxiv.org/abs/2508.04286\nRelevancy score: 8\nNovelty score: 7\nPriority: Must-read\nReasons for match: \ud3ec\uc778\ud2b8\ud074\ub77c\uc6b0\ub4dc \uc815\ud569(point cloud registration)\uc740 3D \ube44\uc804 \ud575\uc2ec \uc8fc\uc81c\uc774\uba70, \uacac\uace0\ud55c PKSS \uae30\ubc18 \uc811\uadfc\uacfc TVCG \ucd9c\ud310\uc740 \uc2e4\ubb34\uc801\u00b7\uc774\ub860\uc801 \uac00\uce58\uac00 \ud07d\ub2c8\ub2e4.\nVenue: IEEE TVCG\nProject page: \n"}, {"main_page": "https://arxiv.org/abs/2508.04335", "pdf": "https://arxiv.org/pdf/2508.04335.pdf", "title": "RiemanLine: Riemannian Manifold Representation of 3D Lines for Factor Graph Optimization", "authors": "Yanyan Li , Ze Yang , Keisuke Tateno , Federico Tombari Liang Zhao , Gim Hee Lee", "subjects": "Computer Vision and Pattern Recognition (cs.CV) ; Robotics (cs.RO)", "abstract": "Minimal parametrization of 3D lines plays a critical role in camera localization and structural mapping. Existing representations in robotics and computer vision predominantly handle independent lines, overlooking structural regularities such as sets of parallel lines that are pervasive in man-made environments. This paper introduces \\textbf{RiemanLine}, a unified minimal representation for 3D lines formulated on Riemannian manifolds that jointly accommodates both individual lines and parallel-line groups. Our key idea is to decouple each line landmark into global and local components: a shared vanishing direction optimized on the unit sphere $\\mathcal{S}^2$, and scaled normal vectors constrained on orthogonal subspaces, enabling compact encoding of structural regularities. For $n$ parallel lines, the proposed representation reduces the parameter space from $4n$ (orthonormal form) to $2n+2$, naturally embedding parallelism without explicit constraints. We further integrate this parameterization into a factor graph framework, allowing global direction alignment and local reprojection optimization within a unified manifold-based bundle adjustment. Extensive experiments on ICL-NUIM, TartanAir, and synthetic benchmarks demonstrate that our method achieves significantly more accurate pose estimation and line reconstruction, while reducing parameter dimensionality and improving convergence stability.", "comments": "", "project_url": "", "Relevancy score": 8, "Novelty score": 7, "Priority": "Must-read", "Reasons for match": "3D \uc120\ud615 \ud45c\ud604\uacfc manifold \ucd5c\uc801\ud654\ub97c \ud1b5\ud574 SLAM/SfM \ubc0f pose estimation\uc5d0 \uc9c1\uc811\uc801\uc778 \uae30\uc5ec\ub97c \ud558\ubbc0\ub85c 3D reconstruction \ubc0f novel representations \uad00\uc2ec\uc0ac\uc5d0 \ub9e4\uc6b0 \uad00\ub828\uc788\uc2b5\ub2c8\ub2e4.", "Venue": "", "Project page": "", "summarized_text": "Title: RiemanLine: Riemannian Manifold Representation of 3D Lines for Factor Graph Optimization\nAuthors: Yanyan Li , Ze Yang , Keisuke Tateno , Federico Tombari Liang Zhao , Gim Hee Lee\nLink: https://arxiv.org/abs/2508.04335\nRelevancy score: 8\nNovelty score: 7\nPriority: Must-read\nReasons for match: 3D \uc120\ud615 \ud45c\ud604\uacfc manifold \ucd5c\uc801\ud654\ub97c \ud1b5\ud574 SLAM/SfM \ubc0f pose estimation\uc5d0 \uc9c1\uc811\uc801\uc778 \uae30\uc5ec\ub97c \ud558\ubbc0\ub85c 3D reconstruction \ubc0f novel representations \uad00\uc2ec\uc0ac\uc5d0 \ub9e4\uc6b0 \uad00\ub828\uc788\uc2b5\ub2c8\ub2e4.\nVenue: \nProject page: \n"}, {"main_page": "https://arxiv.org/abs/2508.04505", "pdf": "https://arxiv.org/pdf/2508.04505.pdf", "title": "MonoCloth: Reconstruction and Animation of Cloth-Decoupled Human Avatars from Monocular Videos", "authors": "Daisheng Jin , Ying He", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "abstract": "Reconstructing realistic 3D human avatars from monocular videos is a challenging task due to the limited geometric information and complex non-rigid motion involved. We present MonoCloth, a new method for reconstructing and animating clothed human avatars from monocular videos. To overcome the limitations of monocular input, we introduce a part-based decomposition strategy that separates the avatar into body, face, hands, and clothing. This design reflects the varying levels of reconstruction difficulty and deformation complexity across these components. Specifically, we focus on detailed geometry recovery for the face and hands. For clothing, we propose a dedicated cloth simulation module that captures garment deformation using temporal motion cues and geometric constraints. Experimental results demonstrate that MonoCloth improves both visual reconstruction quality and animation realism compared to existing methods. Furthermore, thanks to its part-based design, MonoCloth also supports additional tasks such as clothing transfer, underscoring its versatility and practical utility.", "comments": "", "project_url": "", "Relevancy score": 8, "Novelty score": 7, "Priority": "Must-read", "Reasons for match": "monocular video\ub85c\ubd80\ud130 part-based clothed human avatar reconstruction \ubc0f cloth simulation\uc744 \uacb0\ud569\ud558\uc5ec 3D \uc7ac\uad6c\uc131\u00b7\uc560\ub2c8\uba54\uc774\uc158 \ubb38\uc81c\ub97c \ub2e4\ub8e8\uc5b4 3D vision\u00b7reconstruction\u00b7rendering \uc5f0\uad6c\uc5d0 \uc9c1\uc811\uc801\uc774\uace0 \uc2e4\uc6a9\uc801\uc774\ub2e4.", "Venue": "", "Project page": "", "summarized_text": "Title: MonoCloth: Reconstruction and Animation of Cloth-Decoupled Human Avatars from Monocular Videos\nAuthors: Daisheng Jin , Ying He\nLink: https://arxiv.org/abs/2508.04505\nRelevancy score: 8\nNovelty score: 7\nPriority: Must-read\nReasons for match: monocular video\ub85c\ubd80\ud130 part-based clothed human avatar reconstruction \ubc0f cloth simulation\uc744 \uacb0\ud569\ud558\uc5ec 3D \uc7ac\uad6c\uc131\u00b7\uc560\ub2c8\uba54\uc774\uc158 \ubb38\uc81c\ub97c \ub2e4\ub8e8\uc5b4 3D vision\u00b7reconstruction\u00b7rendering \uc5f0\uad6c\uc5d0 \uc9c1\uc811\uc801\uc774\uace0 \uc2e4\uc6a9\uc801\uc774\ub2e4.\nVenue: \nProject page: \n"}, {"main_page": "https://arxiv.org/abs/2508.04567", "pdf": "https://arxiv.org/pdf/2508.04567.pdf", "title": "Analyzing and Mitigating Object Hallucination: A Training Bias Perspective", "authors": "Yifan Li , Kun Zhou , Wayne Xin Zhao , Lei Fang , Ji-Rong Wen", "subjects": "Computer Vision and Pattern Recognition (cs.CV) ; Computation and Language (cs.CL)", "abstract": "As scaling up training data has significantly improved the general multimodal capabilities of Large Vision-Language Models (LVLMs), they still suffer from the hallucination issue, generating text that is inconsistent with the visual input. This phenomenon motivates us to systematically investigate the role of training data in hallucination. We introduce a new benchmark, POPEv2, which consists of counterfactual images collected from the training data of LVLMs with certain objects masked. Through comprehensive evaluation on POPEv2, we find that current LVLMs suffer from training bias: they fail to fully leverage their training data and hallucinate more frequently on images seen during training. Specifically, they perform poorly on counterfactual images, often incorrectly answering ``Yes'' to questions about masked objects. To understand this issue, we conduct probing experiments on the models' internal components, revealing that this training bias is primarily located in the language modeling (LM) head. Based on these findings, we propose Obliviate, an efficient and lightweight unlearning method designed to mitigate object hallucination via training bias unlearning. Obliviate identifies the discrepancy between ground-truth labels and model outputs on the training data as a proxy for bias and adopts a parameter- and data-efficient fine-tuning strategy that only updates the LM head. Extensive experiments demonstrate the effectiveness of our approach. While only reusing the training data and updating approximately 2\\% of the parameters, Obliviate significantly reduces hallucination across both discriminative and generative tasks. Furthermore, it demonstrates strong scalability with respect to both model size (2B to 72B) and training data volume, and exhibits promising generalization to hallucination types beyond object-level hallucination. Our code and data will be publicly released.", "comments": "", "project_url": "", "Relevancy score": 8, "Novelty score": 8, "Priority": "Must-read", "Reasons for match": "Vision-Language Models\uc758 hallucination \ubb38\uc81c\ub97c \ud6c8\ub828 \ub370\uc774\ud130 \ud3b8\ud5a5 \uad00\uc810\uc5d0\uc11c \ubd84\uc11d\ud558\uace0, \uacbd\ub7c9\ud654\ub41c unlearning \uae30\ubc95(\ud2b9\ud788 LM head\ub9cc \uc5c5\ub370\uc774\ud2b8)\uc744 \uc81c\uc548\ud558\uc5ec VLM/VLM \uacc4\uc5f4 \ubaa8\ub378\uc758 \uc2e0\ub8b0\uc131 \ud5a5\uc0c1\uc5d0 \uc9c1\uc811\uc801\uc73c\ub85c \uae30\uc5ec\ud569\ub2c8\ub2e4.", "Venue": "", "Project page": "", "summarized_text": "Title: Analyzing and Mitigating Object Hallucination: A Training Bias Perspective\nAuthors: Yifan Li , Kun Zhou , Wayne Xin Zhao , Lei Fang , Ji-Rong Wen\nLink: https://arxiv.org/abs/2508.04567\nRelevancy score: 8\nNovelty score: 8\nPriority: Must-read\nReasons for match: Vision-Language Models\uc758 hallucination \ubb38\uc81c\ub97c \ud6c8\ub828 \ub370\uc774\ud130 \ud3b8\ud5a5 \uad00\uc810\uc5d0\uc11c \ubd84\uc11d\ud558\uace0, \uacbd\ub7c9\ud654\ub41c unlearning \uae30\ubc95(\ud2b9\ud788 LM head\ub9cc \uc5c5\ub370\uc774\ud2b8)\uc744 \uc81c\uc548\ud558\uc5ec VLM/VLM \uacc4\uc5f4 \ubaa8\ub378\uc758 \uc2e0\ub8b0\uc131 \ud5a5\uc0c1\uc5d0 \uc9c1\uc811\uc801\uc73c\ub85c \uae30\uc5ec\ud569\ub2c8\ub2e4.\nVenue: \nProject page: \n"}, {"main_page": "https://arxiv.org/abs/2508.04597", "pdf": "https://arxiv.org/pdf/2508.04597.pdf", "title": "Pseudo Depth Meets Gaussian: A Feed-forward RGB SLAM Baseline", "authors": "Linqing Zhao , Xiuwei Xu , Yirui Wang , Hao Wang , Wenzhao Zheng , Yansong Tang , Haibin Yan , Jiwen Lu", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "abstract": "Incrementally recovering real-sized 3D geometry from a pose-free RGB stream is a challenging task in 3D reconstruction, requiring minimal assumptions on input data. Existing methods can be broadly categorized into end-to-end and visual SLAM-based approaches, both of which either struggle with long sequences or depend on slow test-time optimization and depth sensors. To address this, we first integrate a depth estimator into an RGB-D SLAM system, but this approach is hindered by inaccurate geometric details in predicted depth. Through further investigation, we find that 3D Gaussian mapping can effectively solve this problem. Building on this, we propose an online 3D reconstruction method using 3D Gaussian-based SLAM, combined with a feed-forward recurrent prediction module to directly infer camera pose from optical flow. This approach replaces slow test-time optimization with fast network inference, significantly improving tracking speed. Additionally, we introduce a local graph rendering technique to enhance robustness in feed-forward pose prediction. Experimental results on the Replica and TUM-RGBD datasets, along with a real-world deployment demonstration, show that our method achieves performance on par with the state-of-the-art SplaTAM, while reducing tracking time by more than 90\\%.", "comments": "IROS 2025", "project_url": "", "Relevancy score": 8, "Novelty score": 8, "Priority": "Must-read", "Reasons for match": "RGB-only feed-forward SLAM with Gaussian mapping \ubc0f fast pose inference\ub294 3D reconstruction, Gaussian Splatting, SLAM \ubc0f real-time tracking \ub4f1 \uc81c \uace0\uc6b0\uc120\uc21c\uc704 \uc5f0\uad6c\uc8fc\uc81c\uc640 \uc9c1\uc811\uc801\uc73c\ub85c \uad00\ub828\ub429\ub2c8\ub2e4.", "Venue": "IROS", "Project page": "", "summarized_text": "Title: Pseudo Depth Meets Gaussian: A Feed-forward RGB SLAM Baseline\nAuthors: Linqing Zhao , Xiuwei Xu , Yirui Wang , Hao Wang , Wenzhao Zheng , Yansong Tang , Haibin Yan , Jiwen Lu\nLink: https://arxiv.org/abs/2508.04597\nRelevancy score: 8\nNovelty score: 8\nPriority: Must-read\nReasons for match: RGB-only feed-forward SLAM with Gaussian mapping \ubc0f fast pose inference\ub294 3D reconstruction, Gaussian Splatting, SLAM \ubc0f real-time tracking \ub4f1 \uc81c \uace0\uc6b0\uc120\uc21c\uc704 \uc5f0\uad6c\uc8fc\uc81c\uc640 \uc9c1\uc811\uc801\uc73c\ub85c \uad00\ub828\ub429\ub2c8\ub2e4.\nVenue: IROS\nProject page: \n"}, {"main_page": "https://arxiv.org/abs/2508.04655", "pdf": "https://arxiv.org/pdf/2508.04655.pdf", "title": "X-SAM: From Segment Anything to Any Segmentation", "authors": "Hao Wang , Limeng Qiao , Zequn Jie , Zhijian Huang , Chengjian Feng , Qingfang Zheng , Lin Ma , Xiangyuan Lan , Xiaodan Liang", "subjects": "Computer Vision and Pattern Recognition (cs.CV) ; Artificial Intelligence (cs.AI)", "abstract": "Large Language Models (LLMs) demonstrate strong capabilities in broad knowledge representation, yet they are inherently deficient in pixel-level perceptual understanding. Although the Segment Anything Model (SAM) represents a significant advancement in visual-prompt-driven image segmentation, it exhibits notable limitations in multi-mask prediction and category-specific segmentation tasks, and it cannot integrate all segmentation tasks within a unified model architecture. To address these limitations, we present X-SAM, a streamlined Multimodal Large Language Model (MLLM) framework that extends the segmentation paradigm from \\textit{segment anything} to \\textit{any segmentation}. Specifically, we introduce a novel unified framework that enables more advanced pixel-level perceptual comprehension for MLLMs. Furthermore, we propose a new segmentation task, termed Visual GrounDed (VGD) segmentation, which segments all instance objects with interactive visual prompts and empowers MLLMs with visual grounded, pixel-wise interpretative capabilities. To enable effective training on diverse data sources, we present a unified training strategy that supports co-training across multiple datasets. Experimental results demonstrate that X-SAM achieves state-of-the-art performance on a wide range of image segmentation benchmarks, highlighting its efficiency for multimodal, pixel-level visual understanding. Code is available at this https URL .", "comments": "Technical Report", "project_url": "", "Relevancy score": 8, "Novelty score": 8, "Priority": "Must-read", "Reasons for match": "SAM\uc744 \ud655\uc7a5\ud574 MLLM\uc758 \ud53d\uc140 \uc218\uc900 \ubd84\ud560 \ub2a5\ub825\uc744 \uac15\ud654\ud558\uace0 \ud1b5\ud569 \ud559\uc2b5 \uc804\ub7b5\uc744 \uc81c\uc2dc\ud558\uc5ec VLM/\uba40\ud2f0\ubaa8\ub2ec \ubc0f \uc138\ubd84\ud654 \uad00\ub828 \uc5f0\uad6c\uc5d0 \uc9c1\uc811\uc801\uc778 \uae30\uc5ec\ub97c \ud569\ub2c8\ub2e4.", "Venue": "Technical Report", "Project page": "https://github.com/", "summarized_text": "Title: X-SAM: From Segment Anything to Any Segmentation\nAuthors: Hao Wang , Limeng Qiao , Zequn Jie , Zhijian Huang , Chengjian Feng , Qingfang Zheng , Lin Ma , Xiangyuan Lan , Xiaodan Liang\nLink: https://arxiv.org/abs/2508.04655\nRelevancy score: 8\nNovelty score: 8\nPriority: Must-read\nReasons for match: SAM\uc744 \ud655\uc7a5\ud574 MLLM\uc758 \ud53d\uc140 \uc218\uc900 \ubd84\ud560 \ub2a5\ub825\uc744 \uac15\ud654\ud558\uace0 \ud1b5\ud569 \ud559\uc2b5 \uc804\ub7b5\uc744 \uc81c\uc2dc\ud558\uc5ec VLM/\uba40\ud2f0\ubaa8\ub2ec \ubc0f \uc138\ubd84\ud654 \uad00\ub828 \uc5f0\uad6c\uc5d0 \uc9c1\uc811\uc801\uc778 \uae30\uc5ec\ub97c \ud569\ub2c8\ub2e4.\nVenue: Technical Report\nProject page: https://github.com/\n"}, {"main_page": "https://arxiv.org/abs/2508.04681", "pdf": "https://arxiv.org/pdf/2508.04681.pdf", "title": "Perceiving and Acting in First-Person: A Dataset and Benchmark for Egocentric Human-Object-Human Interactions", "authors": "Liang Xu , Chengqun Yang , Zili Lin , Fei Xu , Yifan Liu , Congsheng Xu , Yiyi Zhang , Jie Qin , Xingdong Sheng , Yunhui Liu , Xin Jin , Yichao Yan , Wenjun Zeng , Xiaokang Yang", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "abstract": "Learning action models from real-world human-centric interaction datasets is important towards building general-purpose intelligent assistants with efficiency. However, most existing datasets only offer specialist interaction category and ignore that AI assistants perceive and act based on first-person acquisition. We urge that both the generalist interaction knowledge and egocentric modality are indispensable. In this paper, we embed the manual-assisted task into a vision-language-action framework, where the assistant provides services to the instructor following egocentric vision and commands. With our hybrid RGB-MoCap system, pairs of assistants and instructors engage with multiple objects and the scene following GPT-generated scripts. Under this setting, we accomplish InterVLA, the first large-scale human-object-human interaction dataset with 11.4 hours and 1.2M frames of multimodal data, spanning 2 egocentric and 5 exocentric videos, accurate human/object motions and verbal commands. Furthermore, we establish novel benchmarks on egocentric human motion estimation, interaction synthesis, and interaction prediction with comprehensive analysis. We believe that our InterVLA testbed and the benchmarks will foster future works on building AI agents in the physical world.", "comments": "Accepted to ICCV 2025. Project Page: this https URL", "project_url": "", "Relevancy score": 8, "Novelty score": 8, "Priority": "Must-read", "Reasons for match": "\ub300\uaddc\ubaa8 \uc5d0\uace0\uc13c\ud2b8\ub9ad \uba40\ud2f0\ubaa8\ub2ec \ub370\uc774\ud130\uc14b\uacfc \ud589\ub3d9\u00b7\uac1d\uccb4 \uc0c1\ud638\uc791\uc6a9 \ubca4\uce58\ub9c8\ud06c\ub294 3D/\ube44\uc804-\uc5b8\uc5b4 \ubc0f \ud589\ub3d9 \uc608\uce21 \uc5f0\uad6c\uc5d0 \uc911\uc694\ud55c \ub9ac\uc18c\uc2a4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.", "Venue": "ICCV", "Project page": "https://github.com/", "summarized_text": "Title: Perceiving and Acting in First-Person: A Dataset and Benchmark for Egocentric Human-Object-Human Interactions\nAuthors: Liang Xu , Chengqun Yang , Zili Lin , Fei Xu , Yifan Liu , Congsheng Xu , Yiyi Zhang , Jie Qin , Xingdong Sheng , Yunhui Liu , Xin Jin , Yichao Yan , Wenjun Zeng , Xiaokang Yang\nLink: https://arxiv.org/abs/2508.04681\nRelevancy score: 8\nNovelty score: 8\nPriority: Must-read\nReasons for match: \ub300\uaddc\ubaa8 \uc5d0\uace0\uc13c\ud2b8\ub9ad \uba40\ud2f0\ubaa8\ub2ec \ub370\uc774\ud130\uc14b\uacfc \ud589\ub3d9\u00b7\uac1d\uccb4 \uc0c1\ud638\uc791\uc6a9 \ubca4\uce58\ub9c8\ud06c\ub294 3D/\ube44\uc804-\uc5b8\uc5b4 \ubc0f \ud589\ub3d9 \uc608\uce21 \uc5f0\uad6c\uc5d0 \uc911\uc694\ud55c \ub9ac\uc18c\uc2a4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.\nVenue: ICCV\nProject page: https://github.com/\n"}, {"main_page": "https://arxiv.org/abs/2508.04059", "pdf": "https://arxiv.org/pdf/2508.04059.pdf", "title": "Beyond the Visible: Benchmarking Occlusion Perception in Multimodal Large Language Models", "authors": "Zhaochen Liu , Kaiwen Gao , Shuyi Liang , Bin Xiao , Limeng Qiao , Lin Ma , Tingting Jiang", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "abstract": "Occlusion perception, a critical foundation for human-level spatial understanding, embodies the challenge of integrating visual recognition and reasoning. Though multimodal large language models (MLLMs) have demonstrated remarkable capabilities, their performance on occlusion perception remains under-explored. To address this gap, we introduce O-Bench, the first visual question answering (VQA) benchmark specifically designed for occlusion perception. Based on SA-1B, we construct 1,365 images featuring semantically coherent occlusion scenarios through a novel layered synthesis approach. Upon this foundation, we annotate 4,588 question-answer pairs in total across five tailored tasks, employing a reliable, semi-automatic workflow. Our extensive evaluation of 22 representative MLLMs against the human baseline reveals a significant performance gap between current MLLMs and humans, which, we find, cannot be sufficiently bridged by model scaling or thinking process. We further identify three typical failure patterns, including an overly conservative bias, a fragile gestalt prediction, and a struggle with quantitative tasks. We believe O-Bench can not only provide a vital evaluation tool for occlusion perception, but also inspire the development of MLLMs for better visual intelligence. Our benchmark will be made publicly available upon paper publication.", "comments": "", "project_url": "", "Relevancy score": 7, "Novelty score": 7, "Priority": "Skim", "Reasons for match": "MLLM\ub4e4\uc758 \uc2dc\uac01\uc801 \ucd94\ub860 \ud2b9\ud788 occlusion perception \ud3c9\uac00 \ubca4\uce58\ub9c8\ud06c\ub294 vision-language \ubaa8\ub378\uc758 \ud55c\uacc4 \ubd84\uc11d\uacfc \ub370\uc774\ud130\uc14b/\ud3c9\uac00 \uc124\uacc4\uc5d0 \uc9c1\uc811\uc801\uc73c\ub85c \uad00\ub828\ub418\uc5b4 VLM \uc5f0\uad6c\uc790\uc5d0\uac8c \uc720\uc775\ud569\ub2c8\ub2e4.", "Venue": "", "Project page": "", "summarized_text": "Title: Beyond the Visible: Benchmarking Occlusion Perception in Multimodal Large Language Models\nAuthors: Zhaochen Liu , Kaiwen Gao , Shuyi Liang , Bin Xiao , Limeng Qiao , Lin Ma , Tingting Jiang\nLink: https://arxiv.org/abs/2508.04059\nRelevancy score: 7\nNovelty score: 7\nPriority: Skim\nReasons for match: MLLM\ub4e4\uc758 \uc2dc\uac01\uc801 \ucd94\ub860 \ud2b9\ud788 occlusion perception \ud3c9\uac00 \ubca4\uce58\ub9c8\ud06c\ub294 vision-language \ubaa8\ub378\uc758 \ud55c\uacc4 \ubd84\uc11d\uacfc \ub370\uc774\ud130\uc14b/\ud3c9\uac00 \uc124\uacc4\uc5d0 \uc9c1\uc811\uc801\uc73c\ub85c \uad00\ub828\ub418\uc5b4 VLM \uc5f0\uad6c\uc790\uc5d0\uac8c \uc720\uc775\ud569\ub2c8\ub2e4.\nVenue: \nProject page: \n"}, {"main_page": "https://arxiv.org/abs/2508.04182", "pdf": "https://arxiv.org/pdf/2508.04182.pdf", "title": "Hacking Hallucinations of MLLMs with Causal Sufficiency and Necessity", "authors": "Peizheng Guo , Jingyao Wang , Wenwen Qiang , Huijie Guo , Changwen Zheng , Jiahuan Zhou , Gang Hua", "subjects": "Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI)", "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities across vision-language tasks. However, they may suffer from hallucinations--generating outputs that are semantically inconsistent with the input image or text. Through causal analyses, we find that: (i) hallucinations with omission may arise from the failure to adequately capture essential causal factors, and (ii) hallucinations with fabrication are likely caused by the model being misled by non-causal cues. To address these challenges, we propose a novel reinforcement learning framework guided by causal completeness, which jointly considers both causal sufficiency and causal necessity of tokens. Specifically, we evaluate each token's standalone contribution and counterfactual indispensability to define a token-level causal completeness reward. This reward is used to construct a causally informed advantage function within the GRPO optimization framework, encouraging the model to focus on tokens that are both causally sufficient and necessary for accurate generation. Experimental results across various benchmark datasets and tasks demonstrate the effectiveness of our approach, which effectively mitigates hallucinations in MLLMs.", "comments": "", "project_url": "", "Relevancy score": 7, "Novelty score": 8, "Priority": "Must-read", "Reasons for match": "MLLM\uc758 hallucination\uc744 causal \ubd84\uc11d\uc73c\ub85c \ud574\uacb0\ud558\ub294 RL \uae30\ubc18 \ubcf4\uc0c1 \uc124\uacc4\ub294 VLM\uc758 \uc2e0\ub8b0\uc131\u00b7\uc751\ub2f5 \uc815\ud569\uc131 \uac1c\uc120\uc5d0 \uc911\uc694\ud55c \uae30\uc5ec\ub97c \ud560 \uac00\ub2a5\uc131\uc774 \ud07d\ub2c8\ub2e4.", "Venue": "", "Project page": "", "summarized_text": "Title: Hacking Hallucinations of MLLMs with Causal Sufficiency and Necessity\nAuthors: Peizheng Guo , Jingyao Wang , Wenwen Qiang , Huijie Guo , Changwen Zheng , Jiahuan Zhou , Gang Hua\nLink: https://arxiv.org/abs/2508.04182\nRelevancy score: 7\nNovelty score: 8\nPriority: Must-read\nReasons for match: MLLM\uc758 hallucination\uc744 causal \ubd84\uc11d\uc73c\ub85c \ud574\uacb0\ud558\ub294 RL \uae30\ubc18 \ubcf4\uc0c1 \uc124\uacc4\ub294 VLM\uc758 \uc2e0\ub8b0\uc131\u00b7\uc751\ub2f5 \uc815\ud569\uc131 \uac1c\uc120\uc5d0 \uc911\uc694\ud55c \uae30\uc5ec\ub97c \ud560 \uac00\ub2a5\uc131\uc774 \ud07d\ub2c8\ub2e4.\nVenue: \nProject page: \n"}, {"main_page": "https://arxiv.org/abs/2508.04228", "pdf": "https://arxiv.org/pdf/2508.04228.pdf", "title": "LayerT2V: Interactive Multi-Object Trajectory Layering for Video Generation", "authors": "Kangrui Cen , Baixuan Zhao , Yi Xin , Siqi Luo , Guangtao Zhai , Xiaohong Liu", "subjects": "Computer Vision and Pattern Recognition (cs.CV) ; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multimedia (cs.MM)", "abstract": "Controlling object motion trajectories in Text-to-Video (T2V) generation is a challenging and relatively under-explored area, particularly in scenarios involving multiple moving objects. Most community models and datasets in the T2V domain are designed for single-object motion, limiting the performance of current generative models in multi-object tasks. Additionally, existing motion control methods in T2V either lack support for multi-object motion scenes or experience severe performance degradation when object trajectories intersect, primarily due to the semantic conflicts in colliding regions. To address these limitations, we introduce LayerT2V, the first approach for generating video by compositing background and foreground objects layer by layer. This layered generation enables flexible integration of multiple independent elements within a video, positioning each element on a distinct \"layer\" and thus facilitating coherent multi-object synthesis while enhancing control over the generation process. Extensive experiments demonstrate the superiority of LayerT2V in generating complex multi-object scenarios, showcasing 1.4x and 4.5x improvements in mIoU and AP50 metrics over state-of-the-art (SOTA) methods. Project page and code are available at this https URL .", "comments": "Project webpage: this https URL", "project_url": "", "Relevancy score": 7, "Novelty score": 7, "Priority": "Skim", "Reasons for match": "Text-to-Video\uc5d0\uc11c multi-object trajectory \uc81c\uc5b4\ub294 \ube44\ub514\uc624 \ud569\uc131\u00b7\ubaa8\uc158 \uc81c\uc5b4\u00b7layered generation \uc124\uacc4 \ub4f1 \ube44\uc804 \uc0dd\uc131 \ubaa8\ub378\uacfc \uc81c\uc5b4 \uac00\ub2a5\ud55c \uc0dd\uc131 \uc5f0\uad6c\uc5d0 \uad00\ub828\uc131\uc774 \ub192\uc2b5\ub2c8\ub2e4.", "Venue": "", "Project page": "https://github.com/", "summarized_text": "Title: LayerT2V: Interactive Multi-Object Trajectory Layering for Video Generation\nAuthors: Kangrui Cen , Baixuan Zhao , Yi Xin , Siqi Luo , Guangtao Zhai , Xiaohong Liu\nLink: https://arxiv.org/abs/2508.04228\nRelevancy score: 7\nNovelty score: 7\nPriority: Skim\nReasons for match: Text-to-Video\uc5d0\uc11c multi-object trajectory \uc81c\uc5b4\ub294 \ube44\ub514\uc624 \ud569\uc131\u00b7\ubaa8\uc158 \uc81c\uc5b4\u00b7layered generation \uc124\uacc4 \ub4f1 \ube44\uc804 \uc0dd\uc131 \ubaa8\ub378\uacfc \uc81c\uc5b4 \uac00\ub2a5\ud55c \uc0dd\uc131 \uc5f0\uad6c\uc5d0 \uad00\ub828\uc131\uc774 \ub192\uc2b5\ub2c8\ub2e4.\nVenue: \nProject page: https://github.com/\n"}, {"main_page": "https://arxiv.org/abs/2508.04485", "pdf": "https://arxiv.org/pdf/2508.04485.pdf", "title": "QuantVSR: Low-Bit Post-Training Quantization for Real-World Video Super-Resolution", "authors": "Bowen Chai , Zheng Chen , Libo Zhu , Wenbo Li , Yong Guo , Yulun Zhang", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "abstract": "Diffusion models have shown superior performance in real-world video super-resolution (VSR). However, the slow processing speeds and heavy resource consumption of diffusion models hinder their practical application and deployment. Quantization offers a potential solution for compressing the VSR model. Nevertheless, quantizing VSR models is challenging due to their temporal characteristics and high fidelity requirements. To address these issues, we propose QuantVSR, a low-bit quantization model for real-world VSR. We propose a spatio-temporal complexity aware (STCA) mechanism, where we first utilize the calibration dataset to measure both spatial and temporal complexities for each layer. Based on these statistics, we allocate layer-specific ranks to the low-rank full-precision (FP) auxiliary branch. Subsequently, we jointly refine the FP and low-bit branches to achieve simultaneous optimization. In addition, we propose a learnable bias alignment (LBA) module to reduce the biased quantization errors. Extensive experiments on synthetic and real-world datasets demonstrate that our method obtains comparable performance with the FP model and significantly outperforms recent leading low-bit quantization methods. Code is available at: this https URL .", "comments": "", "project_url": "", "Relevancy score": 7, "Novelty score": 7, "Priority": "Must-read", "Reasons for match": "video diffusion models\uc758 low-bit post-training quantization\uc744 \ub2e4\ub8e8\uba70, diffusion \uae30\ubc18 VSR\uc758 \ud6a8\uc728\ud654(quantization, temporal-aware) \ubc0f \uc2e4\ubb34 \ubc30\ud3ec \uad00\ub828 \uc5f0\uad6c\ub85c \ubcf8\uc778\uc758 diffusion/efficiency \uad00\uc2ec\uc0ac\uc5d0 \uc9c1\uc811\uc801 \uad00\ub828\uc774 \uc788\ub2e4.", "Venue": "", "Project page": "https://github.com/", "summarized_text": "Title: QuantVSR: Low-Bit Post-Training Quantization for Real-World Video Super-Resolution\nAuthors: Bowen Chai , Zheng Chen , Libo Zhu , Wenbo Li , Yong Guo , Yulun Zhang\nLink: https://arxiv.org/abs/2508.04485\nRelevancy score: 7\nNovelty score: 7\nPriority: Must-read\nReasons for match: video diffusion models\uc758 low-bit post-training quantization\uc744 \ub2e4\ub8e8\uba70, diffusion \uae30\ubc18 VSR\uc758 \ud6a8\uc728\ud654(quantization, temporal-aware) \ubc0f \uc2e4\ubb34 \ubc30\ud3ec \uad00\ub828 \uc5f0\uad6c\ub85c \ubcf8\uc778\uc758 diffusion/efficiency \uad00\uc2ec\uc0ac\uc5d0 \uc9c1\uc811\uc801 \uad00\ub828\uc774 \uc788\ub2e4.\nVenue: \nProject page: https://github.com/\n"}, {"main_page": "https://arxiv.org/abs/2508.04677", "pdf": "https://arxiv.org/pdf/2508.04677.pdf", "title": "ANPrompt: Anti-noise Prompt Tuning for Vision-Language Models", "authors": "Yansheng Gao , Yufei Zheng , Jinghan Qu , Zixi Zhu , Yukuan Zhang , Shengsheng Wang", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "abstract": "Prompt tuning has emerged as an efficient and effective technique for adapting vision-language models (VLMs) with low computational overhead. However, existing methods often overlook the vulnerability of prompt-tuned VLMs to weak semantic perturbations-such as subtle image or text noise-that degrade their generalization to unseen classes. To address this limitation, we propose ANPrompt, a novel prompt tuning framework designed to enhance robustness under such perturbations. ANPrompt first constructs weak noise text features by fusing original and noise-perturbed text embeddings, which are then clustered to form noise prompts. These noise prompts are integrated with learnable prompt tokens to generate anti-noise prompts, which are injected into the deeper layers of both image and text encoders. To further capture the noise-aware visual semantics, ANPrompt computes the Noise-Resistant Visual Prompt Prototype (NRVPP) by averaging the output prompt tokens from the vision encoder. Finally, ANPrompt introduces alignment, robustness, and anti-noise objectives by computing a Weak semantic noise Alignment Loss (WALoss) alongside the standard cross-entropy and sim loss. Experiments across 11 benchmarks demonstrate that ANPrompt consistently outperforms existing prompt tuning approaches, achieving superior robustness to semantic noise and improved generalization to novel categories.", "comments": "", "project_url": "", "Relevancy score": 7, "Novelty score": 7, "Priority": "Skim", "Reasons for match": "VLM \ud504\ub86c\ud504\ud2b8 \ud29c\ub2dd\uc758 \uacac\uace0\uc131 \ud5a5\uc0c1\uc744 \ub2e4\ub8e8\uc5b4 Vision-Language \ubaa8\ub378\uc758 \uc77c\ubc18\ud654 \ubc0f \uacac\uace0\uc131 \uc5f0\uad6c\uc5d0 \uc9c1\uc811\uc801\uc73c\ub85c \uae30\uc5ec\ud569\ub2c8\ub2e4.", "Venue": "", "Project page": "", "summarized_text": "Title: ANPrompt: Anti-noise Prompt Tuning for Vision-Language Models\nAuthors: Yansheng Gao , Yufei Zheng , Jinghan Qu , Zixi Zhu , Yukuan Zhang , Shengsheng Wang\nLink: https://arxiv.org/abs/2508.04677\nRelevancy score: 7\nNovelty score: 7\nPriority: Skim\nReasons for match: VLM \ud504\ub86c\ud504\ud2b8 \ud29c\ub2dd\uc758 \uacac\uace0\uc131 \ud5a5\uc0c1\uc744 \ub2e4\ub8e8\uc5b4 Vision-Language \ubaa8\ub378\uc758 \uc77c\ubc18\ud654 \ubc0f \uacac\uace0\uc131 \uc5f0\uad6c\uc5d0 \uc9c1\uc811\uc801\uc73c\ub85c \uae30\uc5ec\ud569\ub2c8\ub2e4.\nVenue: \nProject page: \n"}, {"main_page": "https://arxiv.org/abs/2508.04702", "pdf": "https://arxiv.org/pdf/2508.04702.pdf", "title": "BEVCon: Advancing Bird's Eye View Perception with Contrastive Learning", "authors": "Ziyang Leng , Jiawei Yang , Zhicheng Ren , Bolei Zhou", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "abstract": "We present BEVCon, a simple yet effective contrastive learning framework designed to improve Bird's Eye View (BEV) perception in autonomous driving. BEV perception offers a top-down-view representation of the surrounding environment, making it crucial for 3D object detection, segmentation, and trajectory prediction tasks. While prior work has primarily focused on enhancing BEV encoders and task-specific heads, we address the underexplored potential of representation learning in BEV models. BEVCon introduces two contrastive learning modules: an instance feature contrast module for refining BEV features and a perspective view contrast module that enhances the image backbone. The dense contrastive learning designed on top of detection losses leads to improved feature representations across both the BEV encoder and the backbone. Extensive experiments on the nuScenes dataset demonstrate that BEVCon achieves consistent performance gains, achieving up to +2.4% mAP improvement over state-of-the-art baselines. Our results highlight the critical role of representation learning in BEV perception and offer a complementary avenue to conventional task-specific optimizations.", "comments": "", "project_url": "", "Relevancy score": 7, "Novelty score": 6, "Priority": "Skim", "Reasons for match": "BEVCon\uc740 BEV \ud45c\ud604 \ud559\uc2b5\uc744 \uc704\ud55c contrastive learning\uc73c\ub85c 3D/\uc790\ub3d9\uc8fc\ud589 \ubc0f representation learning\uc5d0 \uc9c1\uc811\uc801\uc73c\ub85c \uae30\uc5ec\ud558\uc5ec \ubcf8\uc778\uc758 \uad00\uc2ec\uc0ac\uc640 \ubc00\uc811\ud569\ub2c8\ub2e4.", "Venue": "", "Project page": "", "summarized_text": "Title: BEVCon: Advancing Bird's Eye View Perception with Contrastive Learning\nAuthors: Ziyang Leng , Jiawei Yang , Zhicheng Ren , Bolei Zhou\nLink: https://arxiv.org/abs/2508.04702\nRelevancy score: 7\nNovelty score: 6\nPriority: Skim\nReasons for match: BEVCon\uc740 BEV \ud45c\ud604 \ud559\uc2b5\uc744 \uc704\ud55c contrastive learning\uc73c\ub85c 3D/\uc790\ub3d9\uc8fc\ud589 \ubc0f representation learning\uc5d0 \uc9c1\uc811\uc801\uc73c\ub85c \uae30\uc5ec\ud558\uc5ec \ubcf8\uc778\uc758 \uad00\uc2ec\uc0ac\uc640 \ubc00\uc811\ud569\ub2c8\ub2e4.\nVenue: \nProject page: \n"}, {"main_page": "https://arxiv.org/abs/2508.04050", "pdf": "https://arxiv.org/pdf/2508.04050.pdf", "title": "DOMR: Establishing Cross-View Segmentation via Dense Object Matching", "authors": "Jitong Liao , Yulu Gao , Shaofei Huang , Jialin Gao , Jie Lei , Ronghua Liang , Si Liu", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "abstract": "Cross-view object correspondence involves matching objects between egocentric (first-person) and exocentric (third-person) views. It is a critical yet challenging task for visual understanding. In this work, we propose the Dense Object Matching and Refinement (DOMR) framework to establish dense object correspondences across views. The framework centers around the Dense Object Matcher (DOM) module, which jointly models multiple objects. Unlike methods that directly match individual object masks to image features, DOM leverages both positional and semantic relationships among objects to find correspondences. DOM integrates a proposal generation module with a dense matching module that jointly encodes visual, spatial, and semantic cues, explicitly constructing inter-object relationships to achieve dense matching among objects. Furthermore, we combine DOM with a mask refinement head designed to improve the completeness and accuracy of the predicted masks, forming the complete DOMR framework. Extensive evaluations on the Ego-Exo4D benchmark demonstrate that our approach achieves state-of-the-art performance with a mean IoU of 49.7% on Ego$\\to$Exo and 55.2% on Exo$\\to$Ego. These results outperform those of previous methods by 5.8% and 4.3%, respectively, validating the effectiveness of our integrated approach for cross-view understanding.", "comments": "Accepted by ACM MM 2025", "project_url": "", "Relevancy score": 6, "Novelty score": 6, "Priority": "Skim", "Reasons for match": "\ud06c\ub85c\uc2a4-\ubdf0 \uac1d\uccb4 \ub9e4\uce6d \ubc0f dense correspondence\ub294 multi-view \uc774\ud574\uc640 \uc77c\ubd80 3D/pose \uad00\ub828 \uc751\uc6a9\uc5d0 \uc720\uc758\ubbf8\ud558\uba70, \uac1d\uccb4 \uad00\uacc4 \ubaa8\ub378\ub9c1\uacfc mask refinement\ub294 CV \ubc0f representation learning \uad00\uc2ec\uc0ac\uc640 \uc5f0\uad00\ub429\ub2c8\ub2e4.", "Venue": "ACM MM 2025", "Project page": "", "summarized_text": "Title: DOMR: Establishing Cross-View Segmentation via Dense Object Matching\nAuthors: Jitong Liao , Yulu Gao , Shaofei Huang , Jialin Gao , Jie Lei , Ronghua Liang , Si Liu\nLink: https://arxiv.org/abs/2508.04050\nRelevancy score: 6\nNovelty score: 6\nPriority: Skim\nReasons for match: \ud06c\ub85c\uc2a4-\ubdf0 \uac1d\uccb4 \ub9e4\uce6d \ubc0f dense correspondence\ub294 multi-view \uc774\ud574\uc640 \uc77c\ubd80 3D/pose \uad00\ub828 \uc751\uc6a9\uc5d0 \uc720\uc758\ubbf8\ud558\uba70, \uac1d\uccb4 \uad00\uacc4 \ubaa8\ub378\ub9c1\uacfc mask refinement\ub294 CV \ubc0f representation learning \uad00\uc2ec\uc0ac\uc640 \uc5f0\uad00\ub429\ub2c8\ub2e4.\nVenue: ACM MM 2025\nProject page: \n"}, {"main_page": "https://arxiv.org/abs/2508.04136", "pdf": "https://arxiv.org/pdf/2508.04136.pdf", "title": "UniFGVC: Universal Training-Free Few-Shot Fine-Grained Vision Classification via Attribute-Aware Multimodal Retrieval", "authors": "Hongyu Guo , Kuan Zhu , Xiangzhao Hao , Haiyun Guo , Ming Tang , Jinqiao Wang", "subjects": "Computer Vision and Pattern Recognition (cs.CV) ; Artificial Intelligence (cs.AI)", "abstract": "Few-shot fine-grained visual classification (FGVC) aims to leverage limited data to enable models to discriminate subtly distinct categories. Recent works mostly finetuned the pre-trained visual language models to achieve performance gain, yet suffering from overfitting and weak generalization. To deal with this, we introduce UniFGVC, a universal training-free framework that reformulates few-shot FGVC as multimodal retrieval. First, we propose the Category-Discriminative Visual Captioner (CDV-Captioner) to exploit the open-world knowledge of multimodal large language models (MLLMs) to generate a structured text description that captures the fine-grained attribute features distinguishing closely related classes. CDV-Captioner uses chain-of-thought prompting and visually similar reference images to reduce hallucination and enhance discrimination of generated captions. Using it we can convert each image into an image-description pair, enabling more comprehensive feature representation, and construct the multimodal category templates using few-shot samples for the subsequent retrieval pipeline. Then, off-the-shelf vision and text encoders embed query and template pairs, and FGVC is accomplished by retrieving the nearest template in the joint space. UniFGVC ensures broad compatibility with diverse MLLMs and encoders, offering reliable generalization and adaptability across few-shot FGVC scenarios. Extensive experiments on 12 FGVC benchmarks demonstrate its consistent superiority over prior few-shot CLIP-based methods and even several fully-supervised MLLMs-based approaches.", "comments": "", "project_url": "", "Relevancy score": 6, "Novelty score": 6, "Priority": "Skim", "Reasons for match": "MLLM\uacfc \uba40\ud2f0\ubaa8\ub2ec \uac80\uc0c9\uc744 \uc774\uc6a9\ud55c training-free few-shot FGVC\ub85c VLM\u00b7multimodal retrieval \ubc0f \ud45c\ud604 \ud65c\uc6a9 \uad00\uc810\uc5d0\uc11c \ud765\ubbf8\ub86d\uace0 CLIP-like \ubc29\ubc95\uacfc \uc9c1\uc811 \uad00\ub828\ub429\ub2c8\ub2e4.", "Venue": "", "Project page": "", "summarized_text": "Title: UniFGVC: Universal Training-Free Few-Shot Fine-Grained Vision Classification via Attribute-Aware Multimodal Retrieval\nAuthors: Hongyu Guo , Kuan Zhu , Xiangzhao Hao , Haiyun Guo , Ming Tang , Jinqiao Wang\nLink: https://arxiv.org/abs/2508.04136\nRelevancy score: 6\nNovelty score: 6\nPriority: Skim\nReasons for match: MLLM\uacfc \uba40\ud2f0\ubaa8\ub2ec \uac80\uc0c9\uc744 \uc774\uc6a9\ud55c training-free few-shot FGVC\ub85c VLM\u00b7multimodal retrieval \ubc0f \ud45c\ud604 \ud65c\uc6a9 \uad00\uc810\uc5d0\uc11c \ud765\ubbf8\ub86d\uace0 CLIP-like \ubc29\ubc95\uacfc \uc9c1\uc811 \uad00\ub828\ub429\ub2c8\ub2e4.\nVenue: \nProject page: \n"}, {"main_page": "https://arxiv.org/abs/2508.04161", "pdf": "https://arxiv.org/pdf/2508.04161.pdf", "title": "Audio-Assisted Face Video Restoration with Temporal and Identity Complementary Learning", "authors": "Yuqin Cao , Yixuan Gao , Wei Sun , Xiaohong Liu , Yulun Zhang , Xiongkuo Min", "subjects": "Computer Vision and Pattern Recognition (cs.CV) ; Multimedia (cs.MM); Sound (cs.SD); Audio and Speech Processing (eess.AS)", "abstract": "Face videos accompanied by audio have become integral to our daily lives, while they often suffer from complex degradations. Most face video restoration methods neglect the intrinsic correlations between the visual and audio features, especially in mouth regions. A few audio-aided face video restoration methods have been proposed, but they only focus on compression artifact removal. In this paper, we propose a General Audio-assisted face Video restoration Network (GAVN) to address various types of streaming video distortions via identity and temporal complementary learning. Specifically, GAVN first captures inter-frame temporal features in the low-resolution space to restore frames coarsely and save computational cost. Then, GAVN extracts intra-frame identity features in the high-resolution space with the assistance of audio signals and face landmarks to restore more facial details. Finally, the reconstruction module integrates temporal features and identity features to generate high-quality face videos. Experimental results demonstrate that GAVN outperforms the existing state-of-the-art methods on face video compression artifact removal, deblurring, and super-resolution. Codes will be released upon publication.", "comments": "", "project_url": "", "Relevancy score": 6, "Novelty score": 6, "Priority": "Skim", "Reasons for match": "\uc624\ub514\uc624-\ube44\ub514\uc624\ub97c \uc774\uc6a9\ud55c \uc5bc\uad74 \ube44\ub514\uc624 \ubcf5\uc6d0\uc73c\ub85c \uba40\ud2f0\ubaa8\ub2ec \ubcf4\uc870 \uc2e0\ud638 \ud65c\uc6a9, temporal \ubc0f identity \ud559\uc2b5\uc740 \ube44\ub514\uc624 \ubcf5\uc6d0\u00b7\uba40\ud2f0\ubaa8\ub2ec \ud45c\ud604 \ud559\uc2b5\uc5d0 \uad00\ub828\ub429\ub2c8\ub2e4.", "Venue": "", "Project page": "", "summarized_text": "Title: Audio-Assisted Face Video Restoration with Temporal and Identity Complementary Learning\nAuthors: Yuqin Cao , Yixuan Gao , Wei Sun , Xiaohong Liu , Yulun Zhang , Xiongkuo Min\nLink: https://arxiv.org/abs/2508.04161\nRelevancy score: 6\nNovelty score: 6\nPriority: Skim\nReasons for match: \uc624\ub514\uc624-\ube44\ub514\uc624\ub97c \uc774\uc6a9\ud55c \uc5bc\uad74 \ube44\ub514\uc624 \ubcf5\uc6d0\uc73c\ub85c \uba40\ud2f0\ubaa8\ub2ec \ubcf4\uc870 \uc2e0\ud638 \ud65c\uc6a9, temporal \ubc0f identity \ud559\uc2b5\uc740 \ube44\ub514\uc624 \ubcf5\uc6d0\u00b7\uba40\ud2f0\ubaa8\ub2ec \ud45c\ud604 \ud559\uc2b5\uc5d0 \uad00\ub828\ub429\ub2c8\ub2e4.\nVenue: \nProject page: \n"}, {"main_page": "https://arxiv.org/abs/2508.04175", "pdf": "https://arxiv.org/pdf/2508.04175.pdf", "title": "AD-FM: Multimodal LLMs for Anomaly Detection via Multi-Stage Reasoning and Fine-Grained Reward Optimization", "authors": "Jingyi Liao , Yongyi Su , Rong-Cheng Tu , Zhao Jin , Wenhao Sun , Yiting Li , Dacheng Tao , Xun Xu , Xulei Yang", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "abstract": "While Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities across diverse domains, their application to specialized anomaly detection (AD) remains constrained by domain adaptation challenges. Existing Group Relative Policy Optimization (GRPO) based approaches suffer from two critical limitations: inadequate training data utilization when models produce uniform responses, and insufficient supervision over reasoning processes that encourage immediate binary decisions without deliberative analysis. We propose a comprehensive framework addressing these limitations through two synergistic innovations. First, we introduce a multi-stage deliberative reasoning process that guides models from region identification to focused examination, generating diverse response patterns essential for GRPO optimization while enabling structured supervision over analytical workflows. Second, we develop a fine-grained reward mechanism incorporating classification accuracy and localization supervision, transforming binary feedback into continuous signals that distinguish genuine analytical insight from spurious correctness. Comprehensive evaluation across multiple industrial datasets demonstrates substantial performance improvements in adapting general vision-language models to specialized anomaly detection. Our method achieves superior accuracy with efficient adaptation of existing annotations, effectively bridging the gap between general-purpose MLLM capabilities and the fine-grained visual discrimination required for detecting subtle manufacturing defects and structural irregularities.", "comments": "", "project_url": "", "Relevancy score": 6, "Novelty score": 6, "Priority": "Skim", "Reasons for match": "MLLM\uc744 \uc0b0\uc5c5\uc6a9 anomaly detection\uc5d0 \uc801\uc751\uc2dc\ud0a4\ub294 \ubc29\ubc95\ub860\uc73c\ub85c, \uba40\ud2f0\uc2a4\ud14c\uc774\uc9c0 \ucd94\ub860\uacfc reward \uc124\uacc4\ub294 VLM/representation \uc801\uc751 \ubc0f \ud6a8\uc728\uc801 fine-tuning\uc5d0 \uc2dc\uc0ac\uc810\uc774 \uc788\uc2b5\ub2c8\ub2e4.", "Venue": "", "Project page": "", "summarized_text": "Title: AD-FM: Multimodal LLMs for Anomaly Detection via Multi-Stage Reasoning and Fine-Grained Reward Optimization\nAuthors: Jingyi Liao , Yongyi Su , Rong-Cheng Tu , Zhao Jin , Wenhao Sun , Yiting Li , Dacheng Tao , Xun Xu , Xulei Yang\nLink: https://arxiv.org/abs/2508.04175\nRelevancy score: 6\nNovelty score: 6\nPriority: Skim\nReasons for match: MLLM\uc744 \uc0b0\uc5c5\uc6a9 anomaly detection\uc5d0 \uc801\uc751\uc2dc\ud0a4\ub294 \ubc29\ubc95\ub860\uc73c\ub85c, \uba40\ud2f0\uc2a4\ud14c\uc774\uc9c0 \ucd94\ub860\uacfc reward \uc124\uacc4\ub294 VLM/representation \uc801\uc751 \ubc0f \ud6a8\uc728\uc801 fine-tuning\uc5d0 \uc2dc\uc0ac\uc810\uc774 \uc788\uc2b5\ub2c8\ub2e4.\nVenue: \nProject page: \n"}, {"main_page": "https://arxiv.org/abs/2508.04181", "pdf": "https://arxiv.org/pdf/2508.04181.pdf", "title": "Deeper Inside Deep ViT", "authors": "Sungrae Hong", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "abstract": "There have been attempts to create large-scale structures in vision models similar to LLM, such as ViT-22B. While this research has provided numerous analyses and insights, our understanding of its practical utility remains incomplete. Therefore, we examine how this model structure reacts and train in a local environment. We also highlight the instability in training and make some model modifications to stabilize it. The ViT-22B model, trained from scratch, overall outperformed ViT in terms of performance under the same parameter size. Additionally, we venture into the task of image generation, which has not been attempted in ViT-22B. We propose an image generation architecture using ViT and investigate which between ViT and ViT-22B is a more suitable structure for image generation.", "comments": "8 pages, 5 figures", "project_url": "", "Relevancy score": 6, "Novelty score": 7, "Priority": "Skim", "Reasons for match": "\ub300\uaddc\ubaa8 ViT \uc544\ud0a4\ud14d\ucc98(Deeper ViT)\uc640 \uc548\uc815\ud654/\uc774\ubbf8\uc9c0 \uc0dd\uc131 \uc2e4\ud5d8\uc740 novel architectures \ubc0f \ube44\uc804 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \ud655\uc7a5\uc131 \uc5f0\uad6c\uc640 \uc9c1\uc811\uc801\uc73c\ub85c \uad00\ub828\ub429\ub2c8\ub2e4.", "Venue": "", "Project page": "", "summarized_text": "Title: Deeper Inside Deep ViT\nAuthors: Sungrae Hong\nLink: https://arxiv.org/abs/2508.04181\nRelevancy score: 6\nNovelty score: 7\nPriority: Skim\nReasons for match: \ub300\uaddc\ubaa8 ViT \uc544\ud0a4\ud14d\ucc98(Deeper ViT)\uc640 \uc548\uc815\ud654/\uc774\ubbf8\uc9c0 \uc0dd\uc131 \uc2e4\ud5d8\uc740 novel architectures \ubc0f \ube44\uc804 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \ud655\uc7a5\uc131 \uc5f0\uad6c\uc640 \uc9c1\uc811\uc801\uc73c\ub85c \uad00\ub828\ub429\ub2c8\ub2e4.\nVenue: \nProject page: \n"}, {"main_page": "https://arxiv.org/abs/2508.04197", "pdf": "https://arxiv.org/pdf/2508.04197.pdf", "title": "Gather and Trace: Rethinking Video TextVQA from an Instance-oriented Perspective", "authors": "Yan Zhang , Gangyan Zeng , Daiqing Wu , Huawen Shen , Binbin Li , Yu Zhou , Can Ma , Xiaojun Bi", "subjects": "Computer Vision and Pattern Recognition (cs.CV) ; Artificial Intelligence (cs.AI)", "abstract": "Video text-based visual question answering (Video TextVQA) aims to answer questions by explicitly reading and reasoning about the text involved in a video. Most works in this field follow a frame-level framework which suffers from redundant text entities and implicit relation modeling, resulting in limitations in both accuracy and efficiency. In this paper, we rethink the Video TextVQA task from an instance-oriented perspective and propose a novel model termed GAT (Gather and Trace). First, to obtain accurate reading result for each video text instance, a context-aggregated instance gathering module is designed to integrate the visual appearance, layout characteristics, and textual contents of the related entities into a unified textual representation. Then, to capture dynamic evolution of text in the video flow, an instance-focused trajectory tracing module is utilized to establish spatio-temporal relationships between instances and infer the final answer. Extensive experiments on several public Video TextVQA datasets validate the effectiveness and generalization of our framework. GAT outperforms existing Video TextVQA methods, video-language pretraining methods, and video large language models in both accuracy and inference speed. Notably, GAT surpasses the previous state-of-the-art Video TextVQA methods by 3.86\\% in accuracy and achieves ten times of faster inference speed than video large language models. The source code is available at this https URL .", "comments": "Accepted by 2025 ACM MM", "project_url": "", "Relevancy score": 6, "Novelty score": 6, "Priority": "Skim", "Reasons for match": "\uc774 \ub17c\ubb38\uc740 video text VQA\uc5d0\uc11c instance-oriented representation\uacfc spatio-temporal tracing\uc744 \ud1b5\ud574 \ud6a8\uc728\uc131\uacfc \uc815\ud655\ub3c4\ub97c \uac1c\uc120\ud558\uc5ec \ube44\uc804-\uc5b8\uc5b4 \uba40\ud2f0\ubaa8\ub2ec \ucd94\ub860\uacfc \uc2dc\ud000\uc2a4 \uae30\ubc18 \ube44\uc804 \ubaa8\ub378 \uc124\uacc4\uc5d0 \uc2dc\uc0ac\uc810\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4.", "Venue": "ACM MM", "Project page": "https://github.com/", "summarized_text": "Title: Gather and Trace: Rethinking Video TextVQA from an Instance-oriented Perspective\nAuthors: Yan Zhang , Gangyan Zeng , Daiqing Wu , Huawen Shen , Binbin Li , Yu Zhou , Can Ma , Xiaojun Bi\nLink: https://arxiv.org/abs/2508.04197\nRelevancy score: 6\nNovelty score: 6\nPriority: Skim\nReasons for match: \uc774 \ub17c\ubb38\uc740 video text VQA\uc5d0\uc11c instance-oriented representation\uacfc spatio-temporal tracing\uc744 \ud1b5\ud574 \ud6a8\uc728\uc131\uacfc \uc815\ud655\ub3c4\ub97c \uac1c\uc120\ud558\uc5ec \ube44\uc804-\uc5b8\uc5b4 \uba40\ud2f0\ubaa8\ub2ec \ucd94\ub860\uacfc \uc2dc\ud000\uc2a4 \uae30\ubc18 \ube44\uc804 \ubaa8\ub378 \uc124\uacc4\uc5d0 \uc2dc\uc0ac\uc810\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4.\nVenue: ACM MM\nProject page: https://github.com/\n"}, {"main_page": "https://arxiv.org/abs/2508.04260", "pdf": "https://arxiv.org/pdf/2508.04260.pdf", "title": "Segment Any Vehicle: Semantic and Visual Context Driven SAM and A Benchmark", "authors": "Xiao Wang , Ziwen Wang , Wentao Wu , Anjie Wang , Jiashu Wu , Yantao Pan , Chenglong Li", "subjects": "Computer Vision and Pattern Recognition (cs.CV) ; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "abstract": "With the rapid advancement of autonomous driving, vehicle perception, particularly detection and segmentation, has placed increasingly higher demands on algorithmic performance. Pre-trained large segmentation models, especially Segment Anything Model (SAM), have sparked significant interest and inspired new research directions in artificial intelligence. However, SAM cannot be directly applied to the fine-grained task of vehicle part segmentation, as its text-prompted segmentation functionality is not publicly accessible, and the mask regions generated by its default mode lack semantic labels, limiting its utility in structured, category-specific segmentation tasks. To address these limitations, we propose SAV, a novel framework comprising three core components: a SAM-based encoder-decoder, a vehicle part knowledge graph, and a context sample retrieval encoding module. The knowledge graph explicitly models the spatial and geometric relationships among vehicle parts through a structured ontology, effectively encoding prior structural knowledge. Meanwhile, the context retrieval module enhances segmentation by identifying and leveraging visually similar vehicle instances from training data, providing rich contextual priors for improved generalization. Furthermore, we introduce a new large-scale benchmark dataset for vehicle part segmentation, named VehicleSeg10K, which contains 11,665 high-quality pixel-level annotations across diverse scenes and viewpoints. We conduct comprehensive experiments on this dataset and two other datasets, benchmarking multiple representative baselines to establish a solid foundation for future research and comparison. % Both the dataset and source code of this paper will be released upon acceptance. Both the dataset and source code of this paper will be released on this https URL", "comments": "", "project_url": "", "Relevancy score": 6, "Novelty score": 6, "Priority": "Skim", "Reasons for match": "SAM \uae30\ubc18\uc758 vehicle part segmentation\uacfc \ub300\uaddc\ubaa8 VehicleSeg10K \ub370\uc774\ud130\uc14b \uc81c\uc548\uc740 \ube44\uc804 \ubaa8\ub378\u00b7\uc2dc\ub9e8\ud2f1 \ubd84\ud560\u00b7\ub300\ud615 \ubaa8\ub378 \uc751\uc6a9 \uce21\uba74\uc5d0\uc11c \uc720\uc758\ubbf8\ud558\uba70 VLM/segmentation \uad00\uc2ec\uacfc \uc5f0\uad00\ub428.", "Venue": "", "Project page": "", "summarized_text": "Title: Segment Any Vehicle: Semantic and Visual Context Driven SAM and A Benchmark\nAuthors: Xiao Wang , Ziwen Wang , Wentao Wu , Anjie Wang , Jiashu Wu , Yantao Pan , Chenglong Li\nLink: https://arxiv.org/abs/2508.04260\nRelevancy score: 6\nNovelty score: 6\nPriority: Skim\nReasons for match: SAM \uae30\ubc18\uc758 vehicle part segmentation\uacfc \ub300\uaddc\ubaa8 VehicleSeg10K \ub370\uc774\ud130\uc14b \uc81c\uc548\uc740 \ube44\uc804 \ubaa8\ub378\u00b7\uc2dc\ub9e8\ud2f1 \ubd84\ud560\u00b7\ub300\ud615 \ubaa8\ub378 \uc751\uc6a9 \uce21\uba74\uc5d0\uc11c \uc720\uc758\ubbf8\ud558\uba70 VLM/segmentation \uad00\uc2ec\uacfc \uc5f0\uad00\ub428.\nVenue: \nProject page: \n"}, {"main_page": "https://arxiv.org/abs/2508.04280", "pdf": "https://arxiv.org/pdf/2508.04280.pdf", "title": "Enhancing Vision-Language Model Training with Reinforcement Learning in Synthetic Worlds for Real-World Success", "authors": "George Bredis , Stanislav Dereka , Viacheslav Sinii , Ruslan Rakhimov , Daniil Gavrilov", "subjects": "Machine Learning (cs.LG) ; Artificial Intelligence (cs.AI)", "abstract": "Interactive multimodal agents must convert raw visual observations into coherent sequences of language-conditioned actions -- a capability that current vision-language models (VLMs) still lack. Earlier reinforcement-learning (RL) efforts could, in principle, endow VLMs with such skills, but they have seldom tested whether the learned behaviours generalize beyond their training simulators, and they depend either on brittle hyperparameter tuning or on dense-reward environments with low state variability. We introduce Vision-Language Decoupled Actor-Critic (VL-DAC), a lightweight, hyperparameter-free RL algorithm. VL-DAC applies PPO updates to action tokens while learning value only at the environment-step level: an arrangement, to our knowledge, not previously explored for large VLMs or LLMs. This simple decoupling removes unstable weighting terms and yields faster, more reliable convergence. Training a single VLM with VL-DAC in one inexpensive simulator at a time (MiniWorld, Gym-Cards, ALFWorld, or WebShop) already produces policies that generalize widely: +50\\% relative on BALROG (game-centric agentic control), +5\\% relative on the hardest part of VSI-Bench (spatial planning), and +2\\% on VisualWebBench (web navigation), all without degrading general image understanding accuracy. These results provide the first evidence that a simple RL algorithm can train VLMs entirely in cheap synthetic worlds while delivering measurable gains on real-image agentic, spatial-reasoning, and web-navigation benchmarks.", "comments": "", "project_url": "", "Relevancy score": 6, "Novelty score": 6, "Priority": "Skim", "Reasons for match": "VLM\uc5d0 RL\uc744 \uacb0\ud569\ud574 \uc2dc\ubbac\ub808\uc774\ud130\uc5d0\uc11c \ud559\uc2b5\ud55c \uc815\ucc45\uc774 \uc2e4\uc81c \uc774\ubbf8\uc9c0 \uae30\ubc18 \ubca4\uce58\ub9c8\ud06c\ub85c \uc77c\ubc18\ud654\ub41c\ub2e4\ub294 \uc810\uc740 vision-language \ud559\uc2b5\uacfc \uc5d0\uc774\uc804\ud2b8 \ud589\ub3d9 \ud559\uc2b5\uc5d0 \uc720\uc758\ubbf8\ud569\ub2c8\ub2e4.", "Venue": "", "Project page": "", "summarized_text": "Title: Enhancing Vision-Language Model Training with Reinforcement Learning in Synthetic Worlds for Real-World Success\nAuthors: George Bredis , Stanislav Dereka , Viacheslav Sinii , Ruslan Rakhimov , Daniil Gavrilov\nLink: https://arxiv.org/abs/2508.04280\nRelevancy score: 6\nNovelty score: 6\nPriority: Skim\nReasons for match: VLM\uc5d0 RL\uc744 \uacb0\ud569\ud574 \uc2dc\ubbac\ub808\uc774\ud130\uc5d0\uc11c \ud559\uc2b5\ud55c \uc815\ucc45\uc774 \uc2e4\uc81c \uc774\ubbf8\uc9c0 \uae30\ubc18 \ubca4\uce58\ub9c8\ud06c\ub85c \uc77c\ubc18\ud654\ub41c\ub2e4\ub294 \uc810\uc740 vision-language \ud559\uc2b5\uacfc \uc5d0\uc774\uc804\ud2b8 \ud589\ub3d9 \ud559\uc2b5\uc5d0 \uc720\uc758\ubbf8\ud569\ub2c8\ub2e4.\nVenue: \nProject page: \n"}, {"main_page": "https://arxiv.org/abs/2508.04366", "pdf": "https://arxiv.org/pdf/2508.04366.pdf", "title": "RotatedMVPS: Multi-view Photometric Stereo with Rotated Natural Light", "authors": "Songyun Yang , Yufei Han , Jilong Zhang , Kongming Liang , Peng Yu , Zhaowei Qu , Heng Guo", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "abstract": "Multiview photometric stereo (MVPS) seeks to recover high-fidelity surface shapes and reflectances from images captured under varying views and illuminations. However, existing MVPS methods often require controlled darkroom settings for varying illuminations or overlook the recovery of reflectances and illuminations properties, limiting their applicability in natural illumination scenarios and downstream inverse rendering tasks. In this paper, we propose RotatedMVPS to solve shape and reflectance recovery under rotated natural light, achievable with a practical rotation stage. By ensuring light consistency across different camera and object poses, our method reduces the unknowns associated with complex environment light. Furthermore, we integrate data priors from off-the-shelf learning-based single-view photometric stereo methods into our MVPS framework, significantly enhancing the accuracy of shape and reflectance recovery. Experimental results on both synthetic and real-world datasets demonstrate the effectiveness of our approach.", "comments": "6 pages", "project_url": "", "Relevancy score": 6, "Novelty score": 6, "Priority": "Skim", "Reasons for match": "Multiview photometric stereo\uc5d0\uc11c natural illumination\uc744 \ub2e4\ub8e8\uba70 shape\u00b7reflectance \ubcf5\uc6d0\uc73c\ub85c 3D \ubcf5\uc6d0 \ubc0f inverse rendering \uad00\uc2ec\uc0ac\uc640 \uc9c1\uc811\uc801 \uc5f0\uad00\uc774 \uc788\uc2b5\ub2c8\ub2e4.", "Venue": "", "Project page": "", "summarized_text": "Title: RotatedMVPS: Multi-view Photometric Stereo with Rotated Natural Light\nAuthors: Songyun Yang , Yufei Han , Jilong Zhang , Kongming Liang , Peng Yu , Zhaowei Qu , Heng Guo\nLink: https://arxiv.org/abs/2508.04366\nRelevancy score: 6\nNovelty score: 6\nPriority: Skim\nReasons for match: Multiview photometric stereo\uc5d0\uc11c natural illumination\uc744 \ub2e4\ub8e8\uba70 shape\u00b7reflectance \ubcf5\uc6d0\uc73c\ub85c 3D \ubcf5\uc6d0 \ubc0f inverse rendering \uad00\uc2ec\uc0ac\uc640 \uc9c1\uc811\uc801 \uc5f0\uad00\uc774 \uc788\uc2b5\ub2c8\ub2e4.\nVenue: \nProject page: \n"}, {"main_page": "https://arxiv.org/abs/2508.04427", "pdf": "https://arxiv.org/pdf/2508.04427.pdf", "title": "Decoding the Multimodal Maze: A Systematic Review on the Adoption of Explainability in Multimodal Attention-based Models", "authors": "Md Raisul Kibria , S\u00e9bastien Lafond , Janan Arslan", "subjects": "Machine Learning (cs.LG) ; Artificial Intelligence (cs.AI)", "abstract": "Multimodal learning has witnessed remarkable advancements in recent years, particularly with the integration of attention-based models, leading to significant performance gains across a variety of tasks. Parallel to this progress, the demand for explainable artificial intelligence (XAI) has spurred a growing body of research aimed at interpreting the complex decision-making processes of these models. This systematic literature review analyzes research published between January 2020 and early 2024 that focuses on the explainability of multimodal models. Framed within the broader goals of XAI, we examine the literature across multiple dimensions, including model architecture, modalities involved, explanation algorithms and evaluation methodologies. Our analysis reveals that the majority of studies are concentrated on vision-language and language-only models, with attention-based techniques being the most commonly employed for explanation. However, these methods often fall short in capturing the full spectrum of interactions between modalities, a challenge further compounded by the architectural heterogeneity across domains. Importantly, we find that evaluation methods for XAI in multimodal settings are largely non-systematic, lacking consistency, robustness, and consideration for modality-specific cognitive and contextual factors. Based on these findings, we provide a comprehensive set of recommendations aimed at promoting rigorous, transparent, and standardized evaluation and reporting practices in multimodal XAI research. Our goal is to support future research in more interpretable, accountable, and responsible mulitmodal AI systems, with explainability at their core.", "comments": "", "project_url": "", "Relevancy score": 6, "Novelty score": 4, "Priority": "Skim", "Reasons for match": "\uc774 \ub17c\ubb38\uc740 multimodal attention \uae30\ubc18 \ubaa8\ub378\uc758 explainability\ub97c \uccb4\uacc4\uc801\uc73c\ub85c \uc815\ub9ac\ud558\uc5ec VLM\uc758 \ud574\uc11d\uac00\ub2a5\uc131 \uc5f0\uad6c\uc5d0 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4; \uadf8\ub7ec\ub098 \uc8fc\ub85c \ub9ac\ubdf0 \ub17c\ubb38\uc774\ub77c \uc0c8 \uc544\ud0a4\ud14d\ucc98\ub098 3D/NeRF \uad00\ub828 \uae30\uc5ec\ub294 \uc801\uc2b5\ub2c8\ub2e4.", "Venue": "", "Project page": "", "summarized_text": "Title: Decoding the Multimodal Maze: A Systematic Review on the Adoption of Explainability in Multimodal Attention-based Models\nAuthors: Md Raisul Kibria , S\u00e9bastien Lafond , Janan Arslan\nLink: https://arxiv.org/abs/2508.04427\nRelevancy score: 6\nNovelty score: 4\nPriority: Skim\nReasons for match: \uc774 \ub17c\ubb38\uc740 multimodal attention \uae30\ubc18 \ubaa8\ub378\uc758 explainability\ub97c \uccb4\uacc4\uc801\uc73c\ub85c \uc815\ub9ac\ud558\uc5ec VLM\uc758 \ud574\uc11d\uac00\ub2a5\uc131 \uc5f0\uad6c\uc5d0 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4; \uadf8\ub7ec\ub098 \uc8fc\ub85c \ub9ac\ubdf0 \ub17c\ubb38\uc774\ub77c \uc0c8 \uc544\ud0a4\ud14d\ucc98\ub098 3D/NeRF \uad00\ub828 \uae30\uc5ec\ub294 \uc801\uc2b5\ub2c8\ub2e4.\nVenue: \nProject page: \n"}, {"main_page": "https://arxiv.org/abs/2508.04472", "pdf": "https://arxiv.org/pdf/2508.04472.pdf", "title": "Zero-Residual Concept Erasure via Progressive Alignment in Text-to-Image Model", "authors": "Hongxu Chen , Zhen Wang , Taoran Mei , Lin Li , Bowei Zhu , Runshi Li , Long Chen", "subjects": "Computer Vision and Pattern Recognition (cs.CV) ; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)", "abstract": "Concept Erasure, which aims to prevent pretrained text-to-image models from generating content associated with semantic-harmful concepts (i.e., target concepts), is getting increased attention. State-of-the-art methods formulate this task as an optimization problem: they align all target concepts with semantic-harmless anchor concepts, and apply closed-form solutions to update the model accordingly. While these closed-form methods are efficient, we argue that existing methods have two overlooked limitations: 1) They often result in incomplete erasure due to \"non-zero alignment residual\", especially when text prompts are relatively complex. 2) They may suffer from generation quality degradation as they always concentrate parameter updates in a few deep layers. To address these issues, we propose a novel closed-form method ErasePro: it is designed for more complete concept erasure and better preserving overall generative quality. Specifically, ErasePro first introduces a strict zero-residual constraint into the optimization objective, ensuring perfect alignment between target and anchor concept features and enabling more complete erasure. Secondly, it employs a progressive, layer-wise update strategy that gradually transfers target concept features to those of the anchor concept from shallow to deep layers. As the depth increases, the required parameter changes diminish, thereby reducing deviations in sensitive deep layers and preserving generative quality. Empirical results across different concept erasure tasks (including instance, art style, and nudity erasure) have demonstrated the effectiveness of our ErasePro.", "comments": "", "project_url": "", "Relevancy score": 6, "Novelty score": 6, "Priority": "Skim", "Reasons for match": "\uc774 \ub17c\ubb38\uc740 text-to-image generative \ubaa8\ub378\uc758 concept erasure\uc5d0 \ub300\ud55c closed-form \ucd5c\uc801\ud654\uc640 layer-wise \uc5c5\ub370\uc774\ud2b8 \uc804\ub7b5\uc744 \uc81c\uc548\ud558\uc5ec generative model\uc758 \uc81c\uc5b4\uc640 \ubcf4\uc874(quality) \uce21\uba74\uc5d0\uc11c \uc2e4\uc6a9\uc801\uc774\uba70, VLM/representation \uc870\uc815 \uad00\uc810\uc5d0\uc11c \uc720\uc6a9\ud560 \uc218 \uc788\ub2e4.", "Venue": "", "Project page": "", "summarized_text": "Title: Zero-Residual Concept Erasure via Progressive Alignment in Text-to-Image Model\nAuthors: Hongxu Chen , Zhen Wang , Taoran Mei , Lin Li , Bowei Zhu , Runshi Li , Long Chen\nLink: https://arxiv.org/abs/2508.04472\nRelevancy score: 6\nNovelty score: 6\nPriority: Skim\nReasons for match: \uc774 \ub17c\ubb38\uc740 text-to-image generative \ubaa8\ub378\uc758 concept erasure\uc5d0 \ub300\ud55c closed-form \ucd5c\uc801\ud654\uc640 layer-wise \uc5c5\ub370\uc774\ud2b8 \uc804\ub7b5\uc744 \uc81c\uc548\ud558\uc5ec generative model\uc758 \uc81c\uc5b4\uc640 \ubcf4\uc874(quality) \uce21\uba74\uc5d0\uc11c \uc2e4\uc6a9\uc801\uc774\uba70, VLM/representation \uc870\uc815 \uad00\uc810\uc5d0\uc11c \uc720\uc6a9\ud560 \uc218 \uc788\ub2e4.\nVenue: \nProject page: \n"}, {"main_page": "https://arxiv.org/abs/2508.04539", "pdf": "https://arxiv.org/pdf/2508.04539.pdf", "title": "TopKD: Top-scaled Knowledge Distillation", "authors": "Qi Wang , Jinjia Zhou", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "abstract": "Recent advances in knowledge distillation (KD) predominantly emphasize feature-level knowledge transfer, frequently overlooking critical information embedded within the teacher's logit distributions. In this paper, we revisit logit-based distillation and reveal an underexplored yet critical element: Top-K knowledge. Motivated by this insight, we propose Top-scaled Knowledge Distillation (TopKD), a simple, efficient, and architecture-agnostic framework that significantly enhances logit-based distillation. TopKD consists of two main components: (1) a Top-K Scaling Module (TSM), which adaptively amplifies the most informative logits, and (2) a Top-K Decoupled Loss (TDL), which offers targeted and effective supervision. Notably, TopKD integrates seamlessly into existing KD methods without introducing extra modules or requiring architectural changes. Extensive experiments on CIFAR-100, ImageNet, STL-10, and Tiny-ImageNet demonstrate that TopKD consistently surpasses state-of-the-art distillation methods. Moreover, our method demonstrates substantial effectiveness when distilling Vision Transformers, underscoring its versatility across diverse network architectures. These findings highlight the significant potential of logits to advance knowledge distillation.", "comments": "12 pages, 6 figures, conference, 8 Tables", "project_url": "", "Relevancy score": 6, "Novelty score": 5, "Priority": "Skim", "Reasons for match": "logit \uae30\ubc18 Knowledge Distillation\uacfc ViT \ub300\uc0c1 \uc131\ub2a5 \ud5a5\uc0c1\uc740 Novel Architectures\u00b7Training Efficiency \ubc0f \ubaa8\ub378 \uc555\ucd95 \uad00\uc810\uc5d0\uc11c \uc720\uc6a9\ud558\uba70, \ube44\uc804 \ubaa8\ub378 \uacbd\ub7c9\ud654\u00b7\uc804\uc774\ud559\uc2b5 \uad00\ub828 \uc5f0\uad6c\uc5d0 \uc2e4\ubb34\uc801 \uc801\uc6a9\uc131\uc774 \uc788\uc2b5\ub2c8\ub2e4.", "Venue": "", "Project page": "", "summarized_text": "Title: TopKD: Top-scaled Knowledge Distillation\nAuthors: Qi Wang , Jinjia Zhou\nLink: https://arxiv.org/abs/2508.04539\nRelevancy score: 6\nNovelty score: 5\nPriority: Skim\nReasons for match: logit \uae30\ubc18 Knowledge Distillation\uacfc ViT \ub300\uc0c1 \uc131\ub2a5 \ud5a5\uc0c1\uc740 Novel Architectures\u00b7Training Efficiency \ubc0f \ubaa8\ub378 \uc555\ucd95 \uad00\uc810\uc5d0\uc11c \uc720\uc6a9\ud558\uba70, \ube44\uc804 \ubaa8\ub378 \uacbd\ub7c9\ud654\u00b7\uc804\uc774\ud559\uc2b5 \uad00\ub828 \uc5f0\uad6c\uc5d0 \uc2e4\ubb34\uc801 \uc801\uc6a9\uc131\uc774 \uc788\uc2b5\ub2c8\ub2e4.\nVenue: \nProject page: \n"}, {"main_page": "https://arxiv.org/abs/2508.04576", "pdf": "https://arxiv.org/pdf/2508.04576.pdf", "title": "ConfProBench: A Confidence Evaluation Benchmark for MLLM-Based Process Judges", "authors": "Yue Zhou , Yi Chang , Yuan Wu", "subjects": "Artificial Intelligence (cs.AI)", "abstract": "Reasoning is a critical capability of multimodal large language models (MLLMs) for solving complex multimodal tasks, and judging the correctness of reasoning steps is crucial for improving this capability. Recently, MLLM-based process judges (MPJs) have been widely used to assess the correctness of reasoning steps in multimodal tasks. Therefore, evaluating MPJs is important for identifying their limitations and guiding future improvements. However, existing benchmarks for MPJs mainly focus on tasks such as step correctness classification and reasoning process search, while overlooking a key aspect: whether the confidence scores produced by MPJs at the step level are reliable. To address this gap, we propose ConfProBench, the first comprehensive benchmark designed to systematically evaluate the reliability of step-level confidence scores generated by MPJs. Our benchmark constructs three types of adversarially perturbed reasoning steps: Synonym Substitution, Syntactic Transformation, and Image Perturbation, to test the robustness of MPJ confidence under perturbations. In addition, we introduce three novel evaluation metrics: Confidence Robustness Score (CRS), Confidence Sensitivity Score (CSS), and Confidence Calibration Score (CCS), which evaluate robustness, sensitivity, and calibration, respectively. We evaluate 14 state-of-the-art MLLMs, including both proprietary and open-source models. Experiments reveal limitations in current MPJs' confidence performance and offer competitive baselines to support future research.", "comments": "", "project_url": "", "Relevancy score": 6, "Novelty score": 5, "Priority": "Skim", "Reasons for match": "MLLM\uc758 reasoning \ubc0f confidence \ud3c9\uac00 \ubca4\uce58\ub9c8\ud06c\ub294 VLM/MLLM \uc2e0\ub8b0\uc131 \uc5f0\uad6c\uc5d0 \uc720\uc6a9\ud558\uba70, multimodal reasoning calibration \uad00\ub828 \uc5f0\uad6c\uc5d0 \uad00\uc2ec\uc774 \uc788\ub294 \uacbd\uc6b0 \ucc38\uace0 \uac00\uce58\uac00 \uc788\uc2b5\ub2c8\ub2e4.", "Venue": "", "Project page": "", "summarized_text": "Title: ConfProBench: A Confidence Evaluation Benchmark for MLLM-Based Process Judges\nAuthors: Yue Zhou , Yi Chang , Yuan Wu\nLink: https://arxiv.org/abs/2508.04576\nRelevancy score: 6\nNovelty score: 5\nPriority: Skim\nReasons for match: MLLM\uc758 reasoning \ubc0f confidence \ud3c9\uac00 \ubca4\uce58\ub9c8\ud06c\ub294 VLM/MLLM \uc2e0\ub8b0\uc131 \uc5f0\uad6c\uc5d0 \uc720\uc6a9\ud558\uba70, multimodal reasoning calibration \uad00\ub828 \uc5f0\uad6c\uc5d0 \uad00\uc2ec\uc774 \uc788\ub294 \uacbd\uc6b0 \ucc38\uace0 \uac00\uce58\uac00 \uc788\uc2b5\ub2c8\ub2e4.\nVenue: \nProject page: \n"}, {"main_page": "https://arxiv.org/abs/2508.04604", "pdf": "https://arxiv.org/pdf/2508.04604.pdf", "title": "TURA: Tool-Augmented Unified Retrieval Agent for AI Search", "authors": "Zhejun Zhao , Yuehu Dong , Alley Liu , Lixue Zheng , Pingsheng Liu , Dongdong Shen , Long Xia , Jiashu Zhao , Dawei Yin", "subjects": "Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)", "abstract": "The advent of Large Language Models (LLMs) is transforming search engines into conversational AI search products, primarily using Retrieval-Augmented Generation (RAG) on web corpora. However, this paradigm has significant industrial limitations. Traditional RAG approaches struggle with real-time needs and structured queries that require accessing dynamically generated content like ticket availability or inventory. Limited to indexing static pages, search engines cannot perform the interactive queries needed for such time-sensitive data. Academic research has focused on optimizing RAG for static content, overlooking complex intents and the need for dynamic sources like databases and real-time APIs. To bridge this gap, we introduce TURA (Tool-Augmented Unified Retrieval Agent for AI Search), a novel three-stage framework that combines RAG with agentic tool-use to access both static content and dynamic, real-time information. TURA has three key components: an Intent-Aware Retrieval module to decompose queries and retrieve information sources encapsulated as Model Context Protocol (MCP) Servers, a DAG-based Task Planner that models task dependencies as a Directed Acyclic Graph (DAG) for optimal parallel execution, and a lightweight Distilled Agent Executor for efficient tool calling. TURA is the first architecture to systematically bridge the gap between static RAG and dynamic information sources for a world-class AI search product. Serving tens of millions of users, it leverages an agentic framework to deliver robust, real-time answers while meeting the low-latency demands of a large-scale industrial system.", "comments": "", "project_url": "", "Relevancy score": 6, "Novelty score": 6, "Priority": "Skim", "Reasons for match": "Tool-augmented retrieval\uacfc agentic framework\ub294 retrieval/RAG \ud55c\uacc4\ub97c \uadf9\ubcf5\ud558\ub294 \uc2e4\uc6a9\uc801 \uc544\ud0a4\ud14d\ucc98\ub85c, VLM\uacfc \uac80\uc0c9 \uc5f0\ub3d9 \uc5f0\uad6c\uc5d0 \uc751\uc6a9 \uac00\ub2a5\uc131\uc774 \uc788\uc5b4 \uad00\ub828\uc131\uc774 \uc788\uc2b5\ub2c8\ub2e4.", "Venue": "", "Project page": "", "summarized_text": "Title: TURA: Tool-Augmented Unified Retrieval Agent for AI Search\nAuthors: Zhejun Zhao , Yuehu Dong , Alley Liu , Lixue Zheng , Pingsheng Liu , Dongdong Shen , Long Xia , Jiashu Zhao , Dawei Yin\nLink: https://arxiv.org/abs/2508.04604\nRelevancy score: 6\nNovelty score: 6\nPriority: Skim\nReasons for match: Tool-augmented retrieval\uacfc agentic framework\ub294 retrieval/RAG \ud55c\uacc4\ub97c \uadf9\ubcf5\ud558\ub294 \uc2e4\uc6a9\uc801 \uc544\ud0a4\ud14d\ucc98\ub85c, VLM\uacfc \uac80\uc0c9 \uc5f0\ub3d9 \uc5f0\uad6c\uc5d0 \uc751\uc6a9 \uac00\ub2a5\uc131\uc774 \uc788\uc5b4 \uad00\ub828\uc131\uc774 \uc788\uc2b5\ub2c8\ub2e4.\nVenue: \nProject page: \n"}, {"main_page": "https://arxiv.org/abs/2508.04605", "pdf": "https://arxiv.org/pdf/2508.04605.pdf", "title": "Multitask Learning with Stochastic Interpolants", "authors": "Hugo Negrel , Florentin Coeurdoux , Michael S. Albergo , Eric Vanden-Eijnden", "subjects": "Machine Learning (cs.LG) ; Dynamical Systems (math.DS)", "abstract": "We propose a framework for learning maps between probability distributions that broadly generalizes the time dynamics of flow and diffusion models. To enable this, we generalize stochastic interpolants by replacing the scalar time variable with vectors, matrices, or linear operators, allowing us to bridge probability distributions across multiple dimensional spaces. This approach enables the construction of versatile generative models capable of fulfilling multiple tasks without task-specific training. Our operator-based interpolants not only provide a unifying theoretical perspective for existing generative models but also extend their capabilities. Through numerical experiments, we demonstrate the zero-shot efficacy of our method on conditional generation and inpainting, fine-tuning and posterior sampling, and multiscale modeling, suggesting its potential as a generic task-agnostic alternative to specialized models.", "comments": "", "project_url": "", "Relevancy score": 6, "Novelty score": 7, "Priority": "Skim", "Reasons for match": "\uc774 \ub17c\ubb38\uc740 diffusion/flow \uacc4\uc5f4\uc758 generative \ubaa8\ub378\uc744 \uc77c\ubc18\ud654\ud558\uace0 multi-task/conditional generation\uacfc posterior sampling\uc5d0 \uc801\uc6a9\ud558\uc5ec representation learning\uacfc \ud6a8\uc728\uc801 \uc0dd\uc131(inference) \uce21\uba74\uc5d0\uc11c \ud765\ubbf8\ub85c\uc6b4 \uc544\uc774\ub514\uc5b4\ub97c \uc81c\uc2dc\ud569\ub2c8\ub2e4. CV/3D \uc9c1\uc811 \uc751\uc6a9\uc740 \uc801\uc9c0\ub9cc score-based generative modeling\uacfc \uba40\ud2f0\ud0dc\uc2a4\ud06c operator \uc124\uacc4\uac00 diffusion \uad00\ub828 \uc5f0\uad6c\uc5d0 \uc601\uac10\uc774 \ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4.", "Venue": "", "Project page": "", "summarized_text": "Title: Multitask Learning with Stochastic Interpolants\nAuthors: Hugo Negrel , Florentin Coeurdoux , Michael S. Albergo , Eric Vanden-Eijnden\nLink: https://arxiv.org/abs/2508.04605\nRelevancy score: 6\nNovelty score: 7\nPriority: Skim\nReasons for match: \uc774 \ub17c\ubb38\uc740 diffusion/flow \uacc4\uc5f4\uc758 generative \ubaa8\ub378\uc744 \uc77c\ubc18\ud654\ud558\uace0 multi-task/conditional generation\uacfc posterior sampling\uc5d0 \uc801\uc6a9\ud558\uc5ec representation learning\uacfc \ud6a8\uc728\uc801 \uc0dd\uc131(inference) \uce21\uba74\uc5d0\uc11c \ud765\ubbf8\ub85c\uc6b4 \uc544\uc774\ub514\uc5b4\ub97c \uc81c\uc2dc\ud569\ub2c8\ub2e4. CV/3D \uc9c1\uc811 \uc751\uc6a9\uc740 \uc801\uc9c0\ub9cc score-based generative modeling\uacfc \uba40\ud2f0\ud0dc\uc2a4\ud06c operator \uc124\uacc4\uac00 diffusion \uad00\ub828 \uc5f0\uad6c\uc5d0 \uc601\uac10\uc774 \ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\nVenue: \nProject page: \n"}, {"main_page": "https://arxiv.org/abs/2508.04663", "pdf": "https://arxiv.org/pdf/2508.04663.pdf", "title": "HierarchicalPrune: Position-Aware Compression for Large-Scale Diffusion Models", "authors": "Young D. Kwon , Rui Li , Sijia Li , Da Li , Sourav Bhattacharya , Stylianos I. Venieris", "subjects": "Computer Vision and Pattern Recognition (cs.CV) ; Artificial Intelligence (cs.AI)", "abstract": "State-of-the-art text-to-image diffusion models (DMs) achieve remarkable quality, yet their massive parameter scale (8-11B) poses significant challenges for inferences on resource-constrained devices. In this paper, we present HierarchicalPrune, a novel compression framework grounded in a key observation: DM blocks exhibit distinct functional hierarchies, where early blocks establish semantic structures while later blocks handle texture refinements. HierarchicalPrune synergistically combines three techniques: (1) Hierarchical Position Pruning, which identifies and removes less essential later blocks based on position hierarchy; (2) Positional Weight Preservation, which systematically protects early model portions that are essential for semantic structural integrity; and (3) Sensitivity-Guided Distillation, which adjusts knowledge-transfer intensity based on our discovery of block-wise sensitivity variations. As a result, our framework brings billion-scale diffusion models into a range more suitable for on-device inference, while preserving the quality of the output images. Specifically, when combined with INT4 weight quantisation, HierarchicalPrune achieves 77.5-80.4% memory footprint reduction (e.g., from 15.8 GB to 3.2 GB) and 27.9-38.0% latency reduction, measured on server and consumer grade GPUs, with the minimum drop of 2.6% in GenEval score and 7% in HPSv2 score compared to the original model. Last but not least, our comprehensive user study with 85 participants demonstrates that HierarchicalPrune maintains perceptual quality comparable to the original model while significantly outperforming prior works.", "comments": "", "project_url": "", "Relevancy score": 6, "Novelty score": 7, "Priority": "Skim", "Reasons for match": "\ub300\uaddc\ubaa8 diffusion \ubaa8\ub378\uc758 \uad6c\uc870\uc801 \uc555\ucd95 \ubc0f \ube14\ub85d\ubcc4 \ubbfc\uac10\ub3c4 \ubd84\uc11d\uc740 3D/\uc774\ubbf8\uc9c0 \uc0dd\uc131 \ubc0f \ud6a8\uc728\ud654(\ud2b9\ud788 diffusion \uae30\ubc18 3D \ud569\uc131)\uc5d0 \uc751\uc6a9\ub420 \uc5ec\uc9c0\uac00 \uc788\uc2b5\ub2c8\ub2e4.", "Venue": "", "Project page": "", "summarized_text": "Title: HierarchicalPrune: Position-Aware Compression for Large-Scale Diffusion Models\nAuthors: Young D. Kwon , Rui Li , Sijia Li , Da Li , Sourav Bhattacharya , Stylianos I. Venieris\nLink: https://arxiv.org/abs/2508.04663\nRelevancy score: 6\nNovelty score: 7\nPriority: Skim\nReasons for match: \ub300\uaddc\ubaa8 diffusion \ubaa8\ub378\uc758 \uad6c\uc870\uc801 \uc555\ucd95 \ubc0f \ube14\ub85d\ubcc4 \ubbfc\uac10\ub3c4 \ubd84\uc11d\uc740 3D/\uc774\ubbf8\uc9c0 \uc0dd\uc131 \ubc0f \ud6a8\uc728\ud654(\ud2b9\ud788 diffusion \uae30\ubc18 3D \ud569\uc131)\uc5d0 \uc751\uc6a9\ub420 \uc5ec\uc9c0\uac00 \uc788\uc2b5\ub2c8\ub2e4.\nVenue: \nProject page: \n"}, {"main_page": "https://arxiv.org/abs/2508.04682", "pdf": "https://arxiv.org/pdf/2508.04682.pdf", "title": "TurboTrain: Towards Efficient and Balanced Multi-Task Learning for Multi-Agent Perception and Prediction", "authors": "Zewei Zhou , Seth Z. Zhao , Tianhui Cai , Zhiyu Huang , Bolei Zhou , Jiaqi Ma", "subjects": "Computer Vision and Pattern Recognition (cs.CV)", "abstract": "End-to-end training of multi-agent systems offers significant advantages in improving multi-task performance. However, training such models remains challenging and requires extensive manual design and monitoring. In this work, we introduce TurboTrain, a novel and efficient training framework for multi-agent perception and prediction. TurboTrain comprises two key components: a multi-agent spatiotemporal pretraining scheme based on masked reconstruction learning and a balanced multi-task learning strategy based on gradient conflict suppression. By streamlining the training process, our framework eliminates the need for manually designing and tuning complex multi-stage training pipelines, substantially reducing training time and improving performance. We evaluate TurboTrain on a real-world cooperative driving dataset, V2XPnP-Seq, and demonstrate that it further improves the performance of state-of-the-art multi-agent perception and prediction models. Our results highlight that pretraining effectively captures spatiotemporal multi-agent features and significantly benefits downstream tasks. Moreover, the proposed balanced multi-task learning strategy enhances detection and prediction.", "comments": "ICCV 2025", "project_url": "", "Relevancy score": 6, "Novelty score": 6, "Priority": "Skim", "Reasons for match": "\uba40\ud2f0\uc5d0\uc774\uc804\ud2b8 \uc2dc\uacf5\uac04 \ud504\ub9ac\ud2b8\ub808\uc778\uacfc \uade0\ud615\uc801 \uba40\ud2f0\ud0dc\uc2a4\ud06c \ud559\uc2b5\uc740 \uba40\ud2f0-\ubdf0\u00b7\ub2e4\uc911 \uc5d0\uc774\uc804\ud2b8 perception/prediction \uc2dc\uc2a4\ud15c \uc124\uacc4\uc5d0 \uc720\uc6a9\ud569\ub2c8\ub2e4.", "Venue": "ICCV", "Project page": "", "summarized_text": "Title: TurboTrain: Towards Efficient and Balanced Multi-Task Learning for Multi-Agent Perception and Prediction\nAuthors: Zewei Zhou , Seth Z. Zhao , Tianhui Cai , Zhiyu Huang , Bolei Zhou , Jiaqi Ma\nLink: https://arxiv.org/abs/2508.04682\nRelevancy score: 6\nNovelty score: 6\nPriority: Skim\nReasons for match: \uba40\ud2f0\uc5d0\uc774\uc804\ud2b8 \uc2dc\uacf5\uac04 \ud504\ub9ac\ud2b8\ub808\uc778\uacfc \uade0\ud615\uc801 \uba40\ud2f0\ud0dc\uc2a4\ud06c \ud559\uc2b5\uc740 \uba40\ud2f0-\ubdf0\u00b7\ub2e4\uc911 \uc5d0\uc774\uc804\ud2b8 perception/prediction \uc2dc\uc2a4\ud15c \uc124\uacc4\uc5d0 \uc720\uc6a9\ud569\ub2c8\ub2e4.\nVenue: ICCV\nProject page: \n"}]